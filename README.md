
# Awesome-Social-Agent [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

### ğŸ”¥ğŸ”¥ğŸ”¥ [**AGIç©ºé—´**](https://agispace.feishu.cn/wiki/JkVuwRaXniSDShklwhMc1b2bnId)


## ğŸ“¢ News
[04/20/2025]

ğŸ“¢ 

âœ¨ 

ğŸš€ **What's New in This Update**:
<br>âœ… Updated to include around 100 additional Vid-LLMs and 15 new benchmarks as of June 2024.

Multiple minor updates will follow this major update. And the GitHub repository will be gradually updated soon. We welcome your reading and feedback â¤ï¸

<font size=5><center><b> Table of Contents </b> </center></font>
- [Awesome-Social-Agent ](#awesome-social-agent-)
    - [ğŸ”¥ğŸ”¥ğŸ”¥ AGI](#-agi)
  - [Why we need Vid-LLMs?](#why-we-need-vid-llms)
  - [âœ¨ Survey](#-survey)
  - [ğŸ˜ Vid-LLMs: Models](#-vid-llms-models)
    - [ğŸ“‘ Citation](#-citation)
      - [ğŸ—’ï¸ Taxonomy 1](#ï¸-taxonomy-1)
        - [ğŸ•¹ï¸ Video Analyzer Ã— LLM](#ï¸-video-analyzer--llm)
         
        - [ğŸ‘¾ Video Embedder Ã— LLM](#-analyzer--embedder--llm)

        - [ğŸ§­ (Analyzer + Embedder) Ã— LLM](#-analyzer--embedder--llm)
      - [ğŸ—’ï¸ Taxonomy 2](#ï¸-taxonomy-2)
        - [ğŸ¤– LLM-based Video Agents](#-llm-based-video-agents)
        - [ğŸ¥ Vid-LLM Pretraining](#-vid-llm-pretraining)
        - [ğŸ‘€ Vid-LLM Instruction Tuning](#-vid-llm-instruction-tuning)
        - [ğŸ¦¾ Hybrid Methods](#-hybrid-methods)
        - [Training-free Methods](#-training-free-methods)
  - [Tasks, Datasets, and Benchmarks](#tasks-datasets-and-benchmarks)
      - [Recognition and Anticipation](#recognition-and-anticipation)
      - [Captioning and Description](#captioning-and-description)
      - [Grounding and Retrieval](#grounding-and-retrieval)
      - [Question Answering](#question-answering)
      - [Video Instruction Tuning](#video-instruction-tuning)
        - [Pretraining Dataset](#pretraining-dataset)
        - [Fine-tuning Dataset](#fine-tuning-dataset)
      - [Video-based Large Language Models Benchmark](#video-based-large-language-models-benchmark)
  - [Contributing](#contributing)
    - [ğŸŒŸ Star History](#-star-history)
    - [â™¥ï¸ Contributors](#ï¸-contributors)

## Why we need Vid-LLMs?

![image](./img/tasks.png)

## âœ¨ Survey
| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
| [**Agent AI: Surveying the Horizons of Multimodal Interaction**](https://arxiv.org/abs/2401.03568) |   Agent AI   | 01/2024 |      [code]()       | arXiv |
| [**Safety at Scale: A Comprehensive Survey of Large Model Safety**](https://arxiv.org/abs/2502.05206) |   Safety at Scale   | 03/2024 |      [code]()       | arXiv |


## ğŸ˜ Vid-LLMs: Models 


### ğŸ“‘ Citation

### ğŸ—’ï¸ Taxonomy 1

#### ğŸ•¹ï¸ Video Analyzer Ã— LLM

#### ğŸ‘¾ Video Embedder Ã— LLM

#### ğŸ§­ (Analyzer + Embedder) Ã— LLM

| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |

#### ğŸ¤– LLM-based Video Agents

| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |

#### ğŸ¥ Vid-LLM Pretraining

| Title                                                        |  Model  |  Date   |                        Code                        |  Venue  |
| :----------------------------------------------------------- | :-----: | :-----: | :------------------------------------------------: | :-----: |


#### ğŸ‘€ Vid-LLM Instruction Tuning

| Title                                                        |     Model     |  Date   |                         Code                         | Venue |
| :----------------------------------------------------------- | :-----------: | :-----: | :--------------------------------------------------: | :---: |

#### ğŸ¦¾ Hybrid Methods

| Title                                                        |        Model        |  Date   |                             Code                             | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :----------------------------------------------------------: | :---: |
    
#### ğŸ’ Training-free Methods

| Title                                                        |        Model        |  Date   | Code | Venue |
| :----------------------------------------------------------- | :-----------------: | :-----: | :--: | :---: |

---

## Tasks, Datasets, and Benchmarks

#### Recognition and Anticipation

| Name               |                            Paper                             | Date |                            Link                             |  Venue  |
| :----------------- | :----------------------------------------------------------: | :--: | :---------------------------------------------------------: | :-----: |

#### Captioning and Description
| Name               |                            Paper                             | Date |                            Link                             |  Venue  |
| :----------------- | :----------------------------------------------------------: | :--: | :---------------------------------------------------------: | :-----: |

#### Grounding and Retrieval
| Name               |                            Paper                             | Date |                            Link                             |  Venue  |
| :----------------- | :----------------------------------------------------------: | :--: | :---------------------------------------------------------: | :-----: |

#### Question Answering
| Name               |                            Paper                             | Date |                            Link                             |  Venue  |
| :----------------- | :----------------------------------------------------------: | :--: | :---------------------------------------------------------: | :-----: |

#### Video Instruction Tuning
| Name               |                            Paper                             | Date |                            Link                             |  Venue  |
| :----------------- | :----------------------------------------------------------: | :--: | :---------------------------------------------------------: | :-----: |






## Contributing

We welcome everyone to contribute to this repository and help improve it. You can submit pull requests to add new papers, projects, and helpful materials, or to correct any errors that you may find. Please make sure that your pull requests follow the "Title|Model|Date|Code|Venue" format. Thank you for your valuable contributions!


### ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=yunlong10/Awesome-LLMs-for-Video-Understanding&type=Date)](https://star-history.com/#yunlong10/Awesome-LLMs-for-Video-Understanding&Date)

### â™¥ï¸ Contributors

Our project wouldn't be possible without the contributions of these amazing people! Thank you all for making this project better.

[Feng Kai](https://github.com/fengkaifengkai/18704466567.github.io) @ University of WhuHan \



<a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding/graphs/contributors">
  <img src="https://github.com/fengkaifengkai/AgentSocial" />
</a>

