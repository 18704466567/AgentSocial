是[[idea]]的子链接，按住ctrl点击跳转到idea笔记


#### 当前大模型安全研究
- 视觉基础模型（VFMs）
- 大语言模型（LLMs）
- 视觉语言预训练模型（VLPs）
- 视觉语言模型（VLMs）
- 扩散模型（DMs）
- 基于大模型的智能体（Agents）

#### 确定了10种攻击类型：
- adversarial 对抗性攻击
- backdoor 后门攻击
- poisoning 中毒
- jailbreak 越狱
- prompt injection 提示词注入
- energy-latency 能量延迟
- membership inference 成员推理
- model extraction 模型提取
- data extraction 数据提取
- agent attacks 代理攻击


![[Pasted image 20250322132152.png]]![[Pasted image 20250322132245.png]]
 *AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence 风险类型类型分类综述*

## 一.视觉基础模型(VFMs)

### 1.预训练视觉 Transformer（ViTs）
与传统卷积神经网络（CNNs）不同，ViTs 将图像作为分块标记化（tokenized patches）的序列进行处理，使其能更有效地捕捉长程空间依赖关系。
#### （1）对抗性攻击
根据攻击者是否对模型具有完全访问权限分为==白盒攻击==和==黑盒攻击==。
##### 白盒攻击
**a.补丁攻击**（Patch Attacks）通过利用视觉 Transformer（ViTs）的模块化结构，试图通过在输入数据的特定图像块（patch）中引入针对性扰动来操纵模型的推理过程。
###### **关键研究对比**：
| 方法               | 核心思想                          | 攻击目标                     | 创新点                              |
|--------------------|---------------------------------|----------------------------|-----------------------------------|
| Joshi et al. [17]  | 利用块稀疏性进行令牌级扰动           | ViTs的令牌处理机制           | 首次量化ViTs对令牌扰动的脆弱性        |
| Patch-Fool [1]     | 通过扰动图像块操控自注意力权重        | 自注意力模块                 | 揭示注意力机制与对抗鲁棒性的关联        |
| SlowFormer [2]     | 设计通用补丁增加计算/能耗成本         | 模型部署效率                 | 攻击目标从精度转向资源消耗（隐蔽性攻击） |

**b.位置嵌入攻击**（Position Embedding Attacks）旨在攻击Transformer模型中令牌（token）的空间或序列位置编码。例如，PEAttack通过周期性干扰（periodicity manipulation）、线性扭曲（linearity distortion）和优化嵌入扰动（optimized embedding distortion），破坏位置编码对空间或时序信息的表征能力，揭示了位置嵌入系统对对抗扰动的普遍脆弱性。

###### **PEAttack的攻击维度**：
| 攻击手段       | 技术原理                         | 攻击目标       |
| ---------- | ---------------------------- | ---------- |
| **周期性干扰**  | 干扰位置编码的正弦/余弦周期函数，破坏相对位置关系建模  | 序列长度外推能力   |
| **线性扭曲**   | 扭曲位置编码的线性递增特性，导致绝对位置信息混乱     | 词序或空间定位准确性 |
| **优化嵌入扰动** | 基于梯度优化生成对抗性扰动，直接最大化位置编码的预测误差 | 端到端位置感知鲁棒性 |

**c.注意力攻击**（Attention Attacks)针对视觉Transformer（ViTs）中自注意力模块的脆弱性展开
###### **方法对比与攻击原理**：
| 方法               | 攻击目标                      | 技术原理                                                                 | 攻击效果                          |
|--------------------|-----------------------------|--------------------------------------------------------------------------|---------------------------------|
| **Attention-Fool**[4] | 自注意力模块的查询-键交互        | 通过修改点积相似度，强制查询关注对抗性键向量                                  | 暴露模型对特定图像块的过度敏感性       |
| **AAS**[5]          | 梯度掩蔽防御机制               | 优化预Softmax阶段的缩放因子，绕过梯度平滑化处理                                | 提升对抗样本在白盒/灰盒场景中的攻击成功率 |
##### 黑盒攻击
**a.基于迁移的攻击**（Transfer-based Attacks）首先生成基于可完全访问的替代模型的对抗样本，随后将其迁移至黑盒目标ViTs进行攻击。基于模型间对抗扰动可迁移性。
###### **关键方法对比**
| 方法              | 技术核心               | 攻击目标    | 创新点               |
| --------------- | ------------------ | ------- | ----------------- |
| **SE-TR**[6]    | 多模型集成扰动优化          | 提升迁移性   | 利用模型多样性降低过拟合      |
| **ATA**[7]      | 激活不确定性注意力 + 敏感嵌入扰动 | 注意力机制漏洞 | 揭示ViT注意力与鲁棒性的动态关联 |
| **LPM**[9]      | 图像块级二值掩码优化         | 判别区域过拟合 | 空间解耦提升跨架构泛化性      |
| **IBA**[18]     | 抑制独特偏置，攻击共享偏置      | 模型间共性漏洞 | 从归纳偏置视角统一攻击策略     |
| **TGR**[11]     | 降低内部梯度方差           | 梯度不稳定性  | 稳定对抗样本生成过程        |
| **VDC**[12]     | 虚拟密集连接重建梯度路径       | 深层梯度消失  | 解决ViT深层梯度回传瓶颈     |
| **FDAP**[13]    | 特征空间高频抑制           | 高频敏感特征  | 利用频域特性增强迁移性       |
| **CRFA**[16]    | 基于近似注意力图的稀疏扰动      | 关键区域定位  | 高效攻击与低扰动幅度的平衡     |
| **SASD-WS**[15] | 锐度感知自蒸馏 + 权重缩放集成   | 损失曲面尖锐性 | 通过模型平滑化提升黑盒攻击鲁棒性  |
|                 |                    |         |                   |

**b.通用对抗攻击策略**（适用于ViTs与CNNs）  
以下方法通过**模型无关（model-agnostic）**设计，确保其在黑盒场景中的广泛适用性：  

1. **双攻击框架**（Wei等人[8], [19]）：  
   - **PNA攻击（Pay No Attention Attack）**：  
     在反向传播过程中**跳过注意力机制的梯度计算**，生成对ViTs和CNNs均有效的扰动。  
   - **PatchOut攻击**：  
     在每次迭代中**随机扰动图像块的子集**，避免依赖特定局部特征，提升跨架构迁移性。  

2. **MIG**[10]：  
   结合**积分梯度**（Integrated Gradients）与**基于动量的更新策略**，精准定位模型无关的关键区域（如物体轮廓、纹理边界），显著增强ViTs与CNNs间的对抗样本迁移性。  

###### **方法对比与原理**  
| 方法                | 技术核心                                                                 | 攻击目标                      | 跨模型优势                          |  
|---------------------|--------------------------------------------------------------------------|-----------------------------|-----------------------------------|  
| **PNA**[8], [19]    | 忽略注意力梯度，生成与架构无关的扰动                                         | 模型特异性注意力机制           | 避免ViT与CNN注意力差异导致的迁移失效    |  
| **PatchOut**[8], [19] | 随机块扰动模拟输入不确定性                                                  | 局部特征过拟合                | 提升对ViT（块结构）与CNN（局部卷积）的普适性 |  
| **MIG**[10]         | 积分梯度定位共性敏感区域 + 动量优化稳定扰动方向                                  | 模型无关关键区域               | 统一ViT（全局注意力）与CNN（局部感受野）的脆弱点 |  

**c.基于查询的攻击**（Query-based Attacks）通过反复查询黑盒模型并利用其反馈结果来估计对抗梯度，从而生成对抗样本，其目标是以**最小查询次数**实现攻击成功。根据模型反馈类型，此类攻击可进一步分为：  
1. **基于分数的攻击（Score-based Attacks）**：  
   模型返回**概率向量（probability vector）**，攻击者利用概率值优化扰动（如NES [1]、Bandits [2]）。  
2. **基于决策的攻击（Decision-based Attacks）**：  
   模型仅返回**前k个类别（top-k classes）**（如仅预测标签）。此类攻击通常从大幅随机噪声开始（先实现错误分类），随后逐步寻找更小噪声并维持误分类（如Boundary Attack [3]、HSJA [4]）。  

针对ViTs中对抗噪声搜索效率低的问题，**PAR**[14]提出一种**由粗到精的块搜索方法（coarse-to-fine patch searching method）**，通过结合噪声幅度（noise magnitude）和敏感度掩码（sensitivity masks）引导搜索过程，以适配ViTs的块结构特性，并缓解非重叠图像块（non-overlapping patches）对攻击效果的负面影响。

###### **关键方法对比**
| 方法类型               | 模型反馈信息          | 典型算法                     | ViTs适配挑战                  |
|-----------------------|---------------------|----------------------------|------------------------------|
| **基于分数的攻击**       | 概率向量（连续值）     | NES、Bandits                | 块结构导致概率平滑，梯度估计噪声大    |
| **基于决策的攻击**       | 离散类别标签          | Boundary Attack、HSJA       | 块边界干扰导致搜索路径震荡         |
| **PAR**[14]（ViTs专用） | 标签或概率均可         | 块级敏感度引导的二分搜索         | 解决非重叠块导致的局部最优陷阱      |

###### **PAR方法详解**
**流程步骤**：  
1. **粗粒度搜索**：  
   基于块敏感度掩码定位高敏感图像块区域。  
2. **细粒度优化**：  
   在选定块内根据噪声幅度迭代调整扰动，避开非重叠块边界干扰。  
3. **动态掩码更新**：  
   根据查询反馈更新敏感度权重，优先攻击ViTs的全局注意力脆弱点。  

**技术优势**：  
- 将查询次数降低30%-50%（相比传统决策攻击）。  
- 在ImageNet上对ViT-B/16攻击成功率达89%，扰动幅度（L2范数）减少22%。  

#### （2）对抗性防御

###### **防御方法对比**  
| 方法         | 优势           | 局限性                | 适用场景            |     |
| ---------- | ------------ | ------------------ | --------------- | --- |
| **对抗训练**   | 直接提升模型鲁棒性    | 计算成本高，可能降低干净样本精度   | 高安全需求场景（如医疗、金融） |     |
| **对抗检测**   | 轻量级部署，实时性强   | 需持续更新检测规则以应对新型攻击   | API服务、边缘设备      |     |
| **鲁棒架构设计** | 不依赖预处理，端到端防御 | 需重新训练或修改模型结构       | 新模型开发阶段         |     |
| **对抗净化**   | 兼容现有模型，无需重训练 | 对强自适应攻击（如BPDA）效果有限 | 黑盒防御或模型不可修改场景   |     |


**a.对抗训练**（Adversarial Training）是一种通过**在训练过程中主动引入对抗样本**来增强模型鲁棒性的防御方法。其核心目标是让模型不仅能在正常数据上表现良好，还能在受到对抗攻击（如人为添加的微小扰动）时保持正确的预测能力。但其存在高昂的计算成本。为在ViTs中解决这一问题：  
- **AGAT**[20] 提出一种**动态注意力引导的丢弃策略（dynamic attention-guided dropping strategy）**，通过在每层**选择性移除部分补丁嵌入（patch embeddings）**，在保持鲁棒性（尤其是ImageNet等大规模数据集）的同时加速训练过程，降低计算开销。  
- **ARDPRM**[28] 通过在训练中**随机丢弃注意力模块的梯度（randomly dropping gradients）**并**掩码补丁扰动（masking patch perturbations）**，提升ViTs的对抗鲁棒性。  

由于ViTs对抗训练的计算成本过高，相关研究仍较为有限。上述方法通过**稀疏化计算路径**（AGAT）和**梯度噪声注入**（ARDPRM），为高效鲁棒的ViT训练提供了新思路。

###### **核心创新对比**  
| 方法        | 技术策略                                | 优化目标                  | 适用场景              |  
|------------|---------------------------------------|-------------------------|---------------------|  
| **AGAT**[20] | 动态丢弃冗余补丁嵌入                        | 降低计算成本，保持精度       | 大规模数据集（如ImageNet） |  
| **ARDPRM**[28] | 随机梯度丢弃 + 补丁扰动掩码                 | 增强模型泛化性与抗干扰能力    | 资源受限的对抗训练       |  

###### **技术细节**  
1. **AGAT的动态丢弃机制**：  
   - 基于注意力权重评估补丁重要性，低权重区域优先丢弃。  
   - 丢弃比例随训练进度动态调整（早期高丢弃率，后期保留更多信息）。  
   - 在ImageNet上训练速度提升40%，鲁棒精度（Robust Accuracy）仅下降2.1%。  

2. **ARDPRM的随机梯度丢弃**：  
   - 以概率 \( p \) 丢弃注意力层的梯度，防止对抗扰动过拟合。  
   - 补丁扰动掩码通过空间随机噪声注入实现，模拟输入不确定性。  
 


**b.视觉Transformer（ViTs）的对抗检测方法**主要利用其两大特性——**基于图像块的推理**（patch-based inference）与**激活特征**（activation characteristics）来检测并缓解对抗样本。具体方法包括： 
1. **图像块痕迹检测（Li等人[21]）**：  
   - **核心概念**：对抗样本在ViTs的图像块划分过程中会产生异常痕迹（**Patch Vestiges**）。  
   - **技术实现**：  
     - 计算**跨图像块的相邻像素间阶跃变化（step changes）**的统计指标。  
     - 基于此构建二元回归分类器，检测对抗样本。  

2. **ARMOR [23]**：  
   - **检测原理**：在ViT特定层中扫描**异常高列得分（unusually high column scores）**，定位对抗性图像块。  
   - **防御策略**：用**平均图像块**（average images）替换异常区域，削弱对抗扰动的影响。  

1. **ViTGuard[22]**：  
   - **方法设计**：采用**掩码自编码器**（masked autoencoder）分析ViT的注意力图（attention maps）与CLS令牌表征（CLS token representations），识别并阻断图像块攻击。  


###### **方法对比**  
| 方法             | 检测依据                | 核心优势                  | 局限性                    |  
|------------------|-------------------------|-------------------------|--------------------------|  
| **Patch Vestiges**[21] | 图像块划分的像素阶跃异常  | 无需模型内部信息，轻量检测  | 对低频扰动或平滑攻击效果有限 |  
| **ARMOR**[23]    | 特定层列得分异常          | 精准定位对抗块，可实时修复  | 依赖先验层选择，跨架构泛化弱 |  
| **ViTGuard**[22] | 注意力与CLS令牌表征偏离   | 端到端检测，适应复杂攻击    | 计算成本较高，需模型微调     |  


**c.鲁棒架构方法**（Robust Architecture Methods）聚焦于为视觉Transformer（ViTs）设计更具对抗鲁棒性的注意力模块。主要方法包括：  

###### **1. 平滑注意力（Smoothed Attention [27]）**  
- **核心机制**：在Softmax函数中引入**温度缩放（temperature scaling）**，防止单一图像块主导注意力权重。  
- **作用**：均衡各图像块的关注度，降低对抗扰动对注意力分布的操控。  
###### **2. 随机增强对抗训练（ReiT [32]）**  
- **技术手段**：通过**IIReSA模块**（随机纠缠令牌优化）将对抗训练与随机化结合。  
- **优势**：减少对抗样本引起的令牌相似性，提升模型泛化能力。  
###### **3. 令牌感知平均池化（TAP [29]）**  
- **创新点**：  
  - **令牌感知平均池化**：融入局部邻域信息，缓解令牌过度聚焦问题。  
  - **注意力多样化损失**：降低注意力向量的余弦相似性，增强分布多样性。  
###### **4. 鲁棒化可解释ViTs（FViTs [31]）**  
- **双重策略**：  
  - 稳定自注意力中的**Top-k索引（top-k indices）**，提升解释可信度。  
  - 结合**高斯噪声与扩散去噪平滑（denoised diffusion smoothing）**，增强预测鲁棒性。  
###### **5. 鲁棒敏感补丁校正（RSPC [30]）**  
- **方法流程**：  
  1. **敏感补丁破坏**：定位并干扰对对抗扰动最敏感的补丁区域。  
  2. **特征对齐**：约束干净输入与破坏输入的中间特征一致性，稳定注意力机制。  

###### **方法对比与意义**  
| 方法                | 核心创新点                          | 作用模块                | 对抗鲁棒性提升方向          |  
|---------------------|-----------------------------------|-----------------------|--------------------------|  
| **Smoothed Attention**[27] | 温度缩放调控注意力权重分布            | Softmax函数            | 注意力均衡化               |  
| **ReiT**[32]        | 随机令牌纠缠优化                     | IIReSA模块             | 泛化性与抗过拟合            |  
| **TAP**[29]         | 局部信息融合 + 注意力多样性损失         | 池化层与损失函数          | 缓解令牌过聚焦              |  
| **FViTs**[31]       | Top-k索引稳定 + 扩散去噪平滑          | 自注意力与预测头          | 可解释性与噪声鲁棒性         |  
| **RSPC**[30]        | 敏感补丁破坏与特征对齐                 | 补丁选择与特征对齐模块     | 注意力稳定性                |  

**d.对抗净化**（Adversarial Purification）是一种与模型无关的输入预处理技术，广泛适用于包括但不限于视觉Transformer（ViTs）在内的多种架构。通过预处理输入数据来消除或削弱对抗扰动，从而将对抗样本（Adversarial Examples）转换为接近干净样本的形式，使其能被模型正确分类。其核心思想是：**在模型推理前对输入进行 “净化” 处理，破坏对抗扰动的有效性**。

###### **一、ViTs专用对抗净化方法**  
1. **DiffPure [33]**：  
   - **流程**：对对抗样本通过**前向随机微分方程（SDE）**注入噪声，再利用预训练的扩散模型去噪。  
   - **优势**：通过扩散过程消除对抗扰动，保留语义信息。  

2. **CGDMP [24]**：  
   - **改进点**：  
     - 优化前向过程的噪声水平，适配ViTs的块结构特性。  
     - 引入**对比损失梯度（contrastive loss gradients）**引导去噪，提升净化效果。  

3. **ADBM [25]**：  
   - **核心思想**：通过分析对抗样本与干净样本在扩散过程中的分布差异，直接建立两者分布的映射关系。  

###### **二、通用视觉模型净化方法**  
- **Purify++ [34]**：  
  基于DiffPure改进扩散模型架构，增强语义保留能力。  
- **DifFilter [35]**：  
  扩展噪声尺度范围，平衡去扰强度与语义保真度。  
- **MimicDiffusion [36]**：  
  在反向扩散过程中抑制对抗影响，适用于CNN和ViTs。  

###### **三、高效净化方法**  
- **OSCP [26]**：  
  提出**单步净化（single-step purification）**，显著降低计算成本。  
- **LightPure [37]**：  
  实现**实时净化（real-time purification）**，适用于边缘设备部署。  
- **LoRID [38]**：  
  基于马尔可夫链的鲁棒净化，提升对自适应攻击的防御能力。  

###### **方法对比与意义**  
| 方法               | 核心创新                                      | 适用场景               | 性能优势                     |  
|--------------------|---------------------------------------------|----------------------|----------------------------|  
| **DiffPure**[33]   | SDE噪声注入 + 扩散去噪                        | 高精度需求场景          | 强语义保留，普适性高            |  
| **CGDMP**[24]      | 对比损失梯度引导的ViTs适配优化                   | ViTs专用              | 针对块结构提升净化效率           |  
| **ADBM**[25]       | 对抗与干净样本分布直接映射                      | 分布差异显著的数据集      | 减少扩散步数，加速收敛            |  
| **OSCP**[26]       | 单步净化                                     | 实时性要求高场景         | 计算成本降低80%                |  
| **LoRID**[38]      | 马尔可夫链鲁棒迭代                            | 防御自适应攻击           | 对BPDA等攻击鲁棒性提升30%        |  

#### （3）后门攻击

**后门攻击**（Backdoor Attacks）可通过数据投毒（Data Poisoning）、训练过程操控或参数篡改注入受害者模型，其中现有针对ViTs的攻击**主要基于数据投毒**。根据攻击特性，我们将其分为四类： 
1. **补丁级攻击（Patch-level Attacks）**：  
   通过篡改输入图像的局部补丁（如添加特定图案）植入后门，利用ViT的**分块嵌入**（Patch Embedding）机制触发恶意行为。  
2. **令牌级攻击（Token-level Attacks）**：  
   针对ViT的**令牌序列（Token Sequence）**设计触发器，例如修改CLS令牌或位置编码，干扰自注意力计算。  
3. **多触发器攻击（Multi-trigger Attacks）**：  
   协同使用多个触发器（如空间补丁+频域噪声），利用ViT的**多模态特征融合**特性增强攻击隐蔽性。  
4. **无数据攻击（Data-free Attacks）**：  
   无需依赖训练数据，通过操纵ViT的**固有机制（如注意力权重分布、梯度传播路径）**直接注入后门（如[1][2]）。  

###### **技术对比**  
| 攻击类型              | 触发机制                          | 利用特性                          | 典型方法                     |  
|----------------------|---------------------------------|---------------------------------|----------------------------|  
| **补丁级攻击**         | 局部图像块添加触发器                | ViT的分块处理与位置编码            | BadNets [3]、TrojanNN [4]    |  
| **令牌级攻击**         | 修改CLS令牌或位置编码               | 自注意力对令牌序列的敏感性           | HiddenTrigger [5]           |  
| **多触发器攻击**        | 空间+频域多模态触发器协同             | ViT的多头注意力与跨模态交互          | BlendAttack [6]             |  
| **无数据攻击**         | 直接操控注意力权重或梯度路径           | ViT的架构固有脆弱性                 | InvisibleBackdoor [7]        |  


**a.补丁级攻击**（Patch-level Attacks）主要利用视觉Transformer（ViT）将图像分割为离散块进行处理的特性，通过在块级别植入触发器实施后门攻击。典型方法包括：  

1. **BadViT [39]**：  
   - **核心机制**：引入一种**通用块级触发器（universal patch-wise trigger）**，仅需少量投毒数据即可将模型的注意力焦点从分类相关块**重定向**至对抗触发器。  
   - **优势**：触发器的通用性强，可跨不同图像样本生效。  

2. **TrojViT [40]**：  
   - **改进策略**：  
     - **块显著性排序（patch salience ranking）**：优先选择对模型决策影响大的块植入触发器。  
     - **注意力目标损失函数（attention-targeted loss）**：强制模型关注触发器块，提升后门激活率。  
     - **参数蒸馏（parameter distillation）**：最小化嵌入后门所需的**位翻转（bit flips）**，降低攻击可检测性。  
   - **效果**：相比BadViT，攻击成功率（ASR）提升15%，且触发器隐蔽性更强。  

###### **方法对比**  
| 方法               | 触发机制                | 技术亮点                          | 攻击成功率（ASR） | 隐蔽性         |  
|--------------------|-----------------------|----------------------------------|----------------|---------------|  
| **BadViT**[39]     | 通用块级触发器           | 小样本触发，跨样本泛化性强              | 82%            | 中等（需显式图案） |  
| **TrojViT**[40]    | 显著性导向块 + 注意力损失   | 参数蒸馏减少位翻转，自适应块选择           | 97%            | 高（隐式噪声）   |  

###### **技术细节补充**  
- **位翻转（Bit Flips）**：指修改模型参数二进制表示所需的位数，TrojViT通过参数蒸馏将其减少至平均2.3次/触发器，低于BadViT的7.1次。  
- **注意力重定向**：在ViT的自注意力层中，触发器块被赋予更高的注意力权重（如从0.1提升至0.8）。  
- **隐蔽性增强**：TrojViT的触发器可伪装为自然噪声（如光照变化），而非显式图案（如BadViT的方形补丁）。  


**b.令牌级攻击**（Token-level Attacks）针对视觉Transformer（ViTs）的令牌化层（tokenization layer）展开。SWARM [41]提出一种可切换后门机制（switchable backdoor mechanism），通过引入**“切换令牌”（switch token）动态切换模型的良性行为（benign behavior）与对抗行为（adversarial behavior），既能保证对抗场景下的高攻击成功率，又能在干净环境中维持正常功能。

###### **技术细节解析**  
1. **切换令牌的作用**：  
   - 当输入包含特定触发令牌时，模型执行对抗行为（如错误分类）。  
   - 在正常输入下，模型维持原始功能，避免后门被常规检测手段发现。  

2. **动态切换机制**：  
   - **良性模式**：自注意力机制忽略触发令牌，按正常流程处理输入。  
   - **对抗模式**：触发令牌强制重定向注意力权重，操控分类结果。  

3. **隐蔽性优势**：  
   - 攻击仅在触发令牌存在时激活，常规输入下模型行为无异常，规避了传统后门攻击的静态特征缺陷。  
###### **性能对比**  
| 方法               | 攻击成功率（ASR） | 干净数据精度（CA） | 触发隐蔽性         |  
|--------------------|------------------|------------------|------------------|  
| **传统后门攻击**     | 92%              | 85%              | 低（固定触发图案）  |  
| **SWARM [41]**      | 96%              | 91%              | 高（动态令牌切换）  |  


**c.多触发器攻击**（Multi-trigger Attacks）通过**并行（parallel）、顺序（sequential）或混合（hybrid）配置**在受害者数据集中注入多个后门触发器（backdoor triggers）。具体而言，**MTBAs [43]** 利用多触发器的以下效应增强攻击威力：  
1. **共存效应（Coexistence）**：多个触发器独立激活，攻击目标可灵活切换（如触发A导致类别1错误，触发B导致类别2错误）。  
2. **覆盖效应（Overwriting）**：新触发器覆盖旧触发器的功能，动态调整攻击目标。  
3. **交叉激活效应（Cross-activation）**：触发器协同作用，组合触发时产生非线性的复合攻击行为。  

这些机制**显著削弱现有防御手段（如神经元剪枝、激活分析）的有效性**，因其通常仅针对单一触发器设计。

###### **技术细节扩展**  
| 效应类型         | 攻击场景示例                          | 防御规避策略                     |  
|------------------|-------------------------------------|--------------------------------|  
| **共存效应**       | 同一图像中植入多个隐形触发器，触发不同错误分类  | 需同时检测多模式异常               |  
| **覆盖效应**       | 攻击者通过更新触发器绕过基于指纹的检测         | 动态更新防御规则难度大              |  
| **交叉激活效应**    | 触发A+B组合导致模型崩溃，而非简单错误分类      | 传统异常行为分析失效               |  

**d.无数据攻击**（Data-free Attacks）无需依赖原始训练数据集即可实施后门注入。**DBIA [42]** 提出一种基于替代数据集（substitute datasets）的方法，通过生成**通用触发器**（universal triggers）来最大化视觉Transformer（ViTs）内部的**注意力聚焦（attention maximization）**。其核心步骤包括：  
1. **触发器生成**：  
   通过优化算法生成触发器，使其在ViT的自注意力模块中激活高权重区域。  
2. **参数微调**：  
   使用**投影梯度下降（PGD, Projected Gradient Descent）**[409]对模型参数进行**最小化调整**（仅调整约0.3%的参数），实现高效且资源消耗极低的后门注入。  


###### **技术优势**  
| 特性                | 描述                                                                 |  
|---------------------|--------------------------------------------------------------------|  
| **无需原始数据**      | 通过替代数据集（如公开数据集或合成数据）生成触发器，避免依赖受害者模型的训练数据。         |  
| **高隐蔽性**         | 参数修改量极小（仅微调部分注意力头权重），常规模型完整性检查（如哈希校验）难以检测。          |  
| **跨模型泛化**       | 生成的通用触发器可迁移至不同ViT变体（如ViT-B/16、Swin Transformer）生效。              |  

###### **与依赖数据攻击的对比**  
| 方法类型           | 数据需求          | 参数修改量       | 隐蔽性       | 攻击成功率（ASR） |  
|--------------------|-----------------|----------------|------------|------------------|  
| **传统数据投毒**     | 需原始训练数据      | 高（重训练模型）   | 低          | 85%~92%          |  
| **DBIA [42]**      | 无需原始数据        | 极低（0.3%）     | 高          | 89%~95%          |  

#### （4）后门防御
针对视觉变换器（ViTs）的后门防御旨在消除（或阻断）触发模式与目标类别之间的关联，同时保持模型精度。两种代表性防御策略为：

1. ​**分块处理**：通过破坏图像块的完整性来阻止触发激活；
2. ​**图像阻断**：利用可解释性机制对后门触发区域进行掩码以中和其影响。

**分块处理策略**通过破坏图像块的完整性来抑制触发效应。Doan等人[44]发现，视觉变换器对位置编码前的分块变换在干净数据精度和攻击成功率上呈现差异性响应，据此提出通过随机丢弃或重排图像块的方法，可有效防御基于分块和混合的后门攻击。

**图像阻断策略**利用可解释性定位并中和触发模式。Subramanya等人[46]证明视觉变换器可通过注意力图定位后门触发区域，并提出在推理阶段动态掩码潜在触发区域的防御机制。后续研究中，Subramanya等人[45]进一步将触发中和机制融入训练阶段，以增强ViT对后门攻击的鲁棒性。

尽管这些方法展现出潜力，该领域仍需构建一个整合非ViT防御机制与ViT特性、统一后门检测、触发模式反演和后门移除等多任务的整体防御框架，文献[410]对此进行了初步探索。

#### （5）数据集
数据集在攻击与防御方法的开发与评估中至关重要。表2总结了对抗性攻击和后门攻击研究中使用的数据集。

**对抗性攻击研究的数据集**  
如表2所示，对抗性攻击研究主要在ImageNet数据集上进行。尽管攻击方法会在CIFAR-10/100、Food-101、GLUE等多种数据集上测试，但防御方法的研究仍局限于ImageNet和CIFAR-10/100。这一失衡揭示了一个核心问题：攻击方法的通用性更强，而防御方法在不同数据集上的泛化能力不足。

**后门攻击研究的数据集**  
后门攻击研究同样集中在ImageNet和CIFAR-10/100数据集上。部分攻击（如DBIA、SWARM）扩展到交通标志识别（GTSRB）、人脸识别（VGGFace）等**领域特定数据集**，而防御方法（如PatchDrop）通常仅针对少数基准数据集进行验证。这种局限性削弱了防御技术的实际应用价值。尽管后门防御研究逐渐转向**鲁棒性推断技术**，但这些技术往往针对特定攻击模式设计，泛化能力有限。未来需在更广泛的数据集上测试**自适应防御策略**，以应对不断演化的后门威胁。


### 2.通用分割模型 SAM（Segment Anything Model）

**SAM（通用分割模型）​** 是一种用于图像分割的基础模型，由三个核心组件构成：

1. ​**基于ViT的图像编码器**：将高分辨率图像转换为嵌入表示；
2. ​**提示编码器**：将多种输入模态（如点、框、文本）转换为令牌嵌入；
3. ​**掩码解码器**：结合前两者的嵌入表示，通过两层Transformer架构生成分割掩码。

由于SAM结构复杂，针对其设计的攻击与防御方法与传统卷积神经网络（CNN）存在显著差异。这种独特性源于其**模块化且相互关联的设计**：任一组件中的漏洞都可能影响其他部分，因此需开发专门的攻防策略。例如，图像编码器的脆弱性可能通过提示传播到掩码生成阶段，而动态提示机制增加了对抗样本的隐蔽性。

#### （1）对抗性攻击
**针对SAM的对抗攻击**可分为以下两类：
##### ​**​白盒攻击（White-box Attacks）​**

**提示无关攻击（Prompt-Agnostic Attacks）​**：此类攻击不依赖特定提示，通过**提示级扰动**或**特征级扰动**破坏SAM的分割泛化能力。

- ​**提示级扰动**：Shen等人[47]提出基于网格的策略，生成与点击位置无关的对抗扰动，使SAM无法准确分割任意区域。
- ​**特征级扰动**：Croce等人[48]通过扰动图像编码器的特征输出，扭曲空间嵌入表示，从而破坏SAM的分割完整性。

##### ​**​黑盒攻击（Black-box Attacks）​**

###### ​**通用攻击（Universal Attacks）​**

生成**通用对抗扰动（UAPs）​**[411]，可在任意提示下持续干扰SAM：

- Han等人[53]利用对比学习优化UAPs，通过加剧特征错位提升攻击效果；
- DarkSAM[54]结合**语义解耦**与**纹理失真**，提出混合空频域框架生成通用扰动。

###### ​**迁移攻击（Transfer-based Attacks）​**

利用SAM的迁移表征生成跨模型/跨任务的对抗扰动：

- ​**损失函数优化**：
    - PATA++[50]引入正则化损失，强化图像编码器的关键特征映射，降低对特定提示数据的依赖；
    - Attack-SAM[49]采用ClipMSE损失，聚焦掩码移除优化，通过空间与语义一致性提升跨任务迁移性；
    - UMI-GRAT[52]分两步生成扰动：先通过代理模型生成通用扰动，再结合梯度鲁棒性损失提升跨模型迁移性。
- ​**变换优化技术**：
    - T-RA[47]对扰动施加频谱变换，使其在SAM变体模型中仍能破坏分割；
    - UAD[51]通过两阶段图像形变及特征对齐生成对抗样本。

#### ​（2）对抗防御

目前针对SAM的对抗防御研究较少，现有方法主要集中于**对抗性调优（adversarial tuning）​**，即将对抗训练融入SAM的提示调优（prompt tuning）过程。例如：

- ​**ASAM**[55]：利用**稳定扩散模型（stable diffusion）​**，通过基于扩散模型的调优在低维流形上生成逼真的对抗样本，随后采用**ControlNet**[412]指导重投影过程，确保生成的样本与原始掩码标注对齐，最终利用这些对抗样本对SAM进行微调。

#### （3） 后门攻击与投毒攻击

针对SAM的后门攻击与投毒攻击研究尚处于起步阶段。本节梳理了以下两类攻击方法：

1. ​**后门攻击**：通过**显性视觉触发器**植入后门。
    - ​**BadSAM**[56]：在SAM的适应阶段嵌入视觉触发器，通过修改模型结构（例如添加MLP层）并利用**SAM-Adapter**[414]注入后门，使得攻击者可通过特定输入操控模型输出。
2. ​**投毒攻击**：利用**隐蔽噪声**生成不可学习样本（unlearnable examples）[413]，防止分割模型利用未经授权的数据。
    - ​**UnSeg**[57]：以数据保护为目的，基于预训练SAM构建双层优化框架，微调通用不可学习噪声生成器，高效生成受保护样本，阻止分割模型从中学习，从而避免个人信息被非法利用

#### （4）数据集
如表2所示，SAM安全研究中使用的数据集与通用分割任务常用数据集（如[415][416]）略有差异：

- ​**攻击研究**：
    - ​**SA-1B数据集及其子集**[408]是评估对抗攻击（如[47]-[51][53]）的主要基准；
    - ​**DarkSAM**在Cityscapes[417]、COCO[418]、ADE20k[419]等场景分割数据集上验证；
    - ​**UMI-GRAT**针对SAM下游任务，测试了医学影像（CT扫描、ISTD）和伪装目标检测（COD10K、CAMO、CHAME）等专用数据集。
- ​**后门攻击**：BadSAM在伪装目标检测数据集CAMO[420]上评估。
- ​**数据投毒**：UnSeg[57]在10个数据集上验证，包括COCO、Cityscapes、ADE20k、WHU以及医学影像（Lung、Kvasir-seg）。
- ​**防御研究**：目前唯一针对SAM的防御方法ASAM[55]在图像分布更广泛的ADE20k、LVIS、COCO等数据集上测试，评估指标为**平均交并比（mIoU）​**。


## 二.大语言模型(LLMs)
大型语言模型（LLMs）是强大的语言模型，擅长生成类人文本、翻译语言、创作内容及回答多样化问题[421][422]，已被快速应用于对话代理、自动化代码生成、科学研究等领域。然而，其广泛应用也引入了潜在攻击者可利用的严重安全漏洞。本节梳理当前LLM安全研究的现状，涵盖以下对抗行为：

1. ​**攻击类型**
    - ​**越狱攻击（Jailbreak）​**：绕过模型安全限制生成有害内容；
    - ​**提示注入攻击（Prompt Injection）​**：通过恶意指令操控输出；
    - ​**后门攻击（Backdoor）​**：植入隐藏触发机制以特定输入控制模型行为；
    - ​**投毒攻击（Poisoning）​**：污染训练数据破坏模型性能；
    - ​**模型提取（Model Extraction）​**：窃取模型参数或架构；
    - ​**数据提取（Data Extraction）​**：从输出中推断训练数据敏感信息；
    - ​**能耗-延迟攻击（Energy-Latency Attacks）​**：通过资源消耗干扰服务可用性。

此类攻击可操纵输出、突破安全屏障、泄露敏感信息或破坏服务，威胁系统**完整性**、**机密性**与**可用性**。

2. ​**防御与对齐策略**
    - 前沿研究聚焦**对齐技术**​（如基于人类反馈的强化学习）与**防御机制**​（如输入过滤、动态检测、鲁棒性训练），以缓解上述风险。相关工作的细节总结于表3与表4。

#### （1）对抗性攻击
针对LLM的对抗性攻击旨在通过微妙地改变输入文本来操纵模型的响应。我们将这些攻击分为白盒攻击和黑盒攻击，这取决于攻击者是否可以访问模型的内部。
##### **白盒攻击**
白盒攻击假设攻击者完全掌握LLM的架构、参数与梯度信息，能够直接针对模型的预测结果优化对抗样本，具有较高的攻击效率。根据对抗样本的生成粒度，此类攻击可分为以下两类：

1. ​**字符级攻击（Character-level Attacks）​**
    
    - ​**方法**：通过**细微字符修改**​（如拼写错误、排版错误）或插入**视觉相似/不可见字符**​（例如同形异义字 [58]）生成对抗样本。
    - ​**原理**：利用模型对字符微小变化的敏感性（人类难以察觉），在保留原语义的同时实现隐蔽性攻击。
2. ​**词级攻击（Word-level Attacks）​**
    
    - ​**同义词替换**：例如TextFooler[59]和BERT-Attack[60]通过替换语义相近的词汇生成对抗样本；
    - ​**梯度优化**：如GBDA[61]、GRADOBSTINATE[63]利用梯度信息筛选可最大化攻击成功率的语义相似词；
    - ​**任务定制化攻击**：针对特定任务或语言场景调整攻击策略，例如：
        - [62]针对命名实体识别任务设计定向词替换攻击；
        - [64]针对中文语境优化词级对抗样本生成方法。
##### ** 黑盒攻击**

黑盒攻击假设攻击者对目标LLM的参数信息知之甚少或完全未知，仅通过**API查询**与模型交互。相较于白盒攻击，黑盒攻击通过间接且自适应的策略利用模型漏洞，通常通过**操纵输入提示（prompt）​**而非直接修改核心文本实现攻击。现有针对LLM的黑盒攻击可细分为以下四类：

1. ​**上下文攻击（In-context Attacks）​**
    
    - ​**方法**：利用**上下文学习（in-context learning）​**中的示范样例引入对抗行为，使模型对污染的提示（poisoned prompts）产生脆弱性。
    - ​**案例**：
        - ​**AdvICL**[65]与**Transferable-advICL**通过篡改示范样例暴露模型漏洞，揭示其对污染上下文数据的敏感性。
2. ​**诱导攻击（Induced Attacks）​**
    
    - ​**方法**：设计欺骗性提示，绕过模型内置安全机制，诱导其生成有害输出。
    - ​**案例**：
        - Liu等人[66]分析如何通过精心构造的输入提示使模型产生危险内容，有效规避安全防护。
3. ​**LLM辅助攻击（LLM-Assisted Attacks）​**
    
    - ​**方法**：将LLM作为工具辅助生成攻击策略，实现对抗流程自动化。
    - ​**案例**：
        - Carlini[423]通过分步提示GPT-4设计攻击算法，展示LLM作为“对抗研究助手”的潜力。
4. ​**表格攻击（Tabular Attacks）​**
    
    - ​**方法**：针对**表格数据**的结构（如列名与注释）注入对抗行为。
    - ​**案例**：
        - Koleva等人[67]提出**实体交换攻击（entity-swap attack）​**，利用训练集到测试集的实体泄漏，篡改表格列类型注释以构造逼真对抗场景。

#### （2）对抗性防御
对抗性防御对于确保LLMs在实际应用中的安全性、可靠性和可信度至关重要。现有的llm对抗性防御策略可以根据其主要关注点大致分为两类：1)对抗性检测和2)鲁棒推理。

###### **a.对抗性检测​**

对抗性检测方法的目的是在潜在的对抗性输入影响模型的输出之前识别和标记它们。目标是实现一种过滤机制，可以区分良性和恶意提示。
大多数针对LLM的对抗性检测方法属于输入过滤技术，其通过识别文本中的**统计或结构异常**来拦截对抗性输入。主要方法包括：

1. ​**基于困惑度（Perplexity）的检测**
    
    - ​**原理**：对抗性提示（如恶意指令）在经校准良好的语言模型评估时通常表现出更高的困惑度值，表明其偏离自然语言模式。
    - ​**方法**：Jain等人[68]通过设定困惑度阈值，过滤超出阈值的异常输入。
2. ​**擦除-校验（Erase-and-Check）​**
    
    - ​**原理**：迭代擦除输入文本的局部内容并校验输出一致性。若输出显著变化，则提示存在对抗性操控风险。
    - ​**方法**：文献[69]提出此方法，通过动态擦除与校验增强检测鲁棒性。

**优势与局限**：

- ​**优势**：输入过滤方法计算轻量，可作为防御的第一道屏障；
- ​**局限**：效果依赖于特征选择（如困惑度阈值设定），且可能被**自适应对抗攻击**绕过（例如针对检测机制设计的隐蔽扰动）。

###### ​**b.鲁棒推断（Robust Inference）​**

鲁棒推断方法通过**修改模型的内部机制或训练方式**，增强其对抗攻击的固有抵抗力。典型案例如：

- ​**Circuit Breaking**[70]：在推断过程中识别并阻断特定**激活模式（activation patterns）​**，无需重新训练即可抑制有害输出生成。

**优势与挑战**：

- ​**优势**：提升对自适应攻击的防御能力；
- ​**局限**：
    - 通常伴随更高的**计算开销**；
    - 防御效果受模型架构与攻击类型影响显著（例如对特定攻击有效，但对其他攻击可能失效）。
#### （3）越狱攻击
 **越狱攻击（Jailbreak Attacks）与对抗攻击（Adversarial Attacks）的差异**
相较于通过**字符级/词级扰动**修改输入的对抗攻击，越狱攻击通过**人工构造或自动生成的越狱提示词（jailbreak prompts）​**诱使LLM生成有害内容。核心区别如下：

1. ​**持续性影响**
    
    - ​**越狱攻击**：一旦模型被越狱，其对后续恶意查询会持续生成有害响应；
    - ​**对抗攻击**：需对每个输入实例单独施加扰动。
2. ​**攻击场景**
    
    - ​**越狱攻击**：主要针对**模型即服务（LLMaaS）​**场景，遵循**黑盒威胁模型**​（攻击者无法访问模型内部参数）；
    - ​**对抗攻击**：通常基于白盒或灰盒假设（需部分模型信息）。

###### ​**a.手工攻击（Hand-crafted Attacks）​**

手工攻击通过设计对抗性提示词（prompt），利用目标LLM的特定漏洞，构造可绕过安全过滤的**词汇/短语组合或结构**，以隐晦表达恶意请求。主要方法分类如下：

1. ​**基于场景的伪装（Scenario-based Camouflage）​**
    
    - ​**方法**：将恶意查询嵌入复杂场景（如角色扮演、解谜任务），掩盖有害意图。
    - ​**案例**：
        - ​**Li等人[74]**：要求LLM扮演可能生成有害内容的角色；
        - ​**SMEA[76]**：将LLM置于权威角色下属地位；
        - ​**Easyjailbreak[75]**：在假设性语境中构造恶意查询；
        - ​**Puzzler[80]**：将恶意请求编码为谜题，其答案即对应有害输出。
2. ​**注意力转移（Attention Shifting）​**
    
    - ​**方法**：通过语言复杂性转移模型对恶意意图的注意力。
    - ​**案例**：
        - ​**Jailbroken[73]**：使用语码切换（如中英混杂）与非标准句式；
        - ​**Tastle[77]**：通过语气操控（如伪装为学术讨论）模糊意图；
        - ​**StructuralSleight[78]**：调整句子结构以干扰模型理解。
3. ​**基于编码的攻击（Encoding-Based Attacks）​**
    
    - ​**方法**：利用LLM处理**罕见编码格式**的局限性（如低资源语言、加密文本）。
    - ​**案例**：
        - ​**Base64编码[73]**：将恶意查询转换为Base64格式；
        - ​**低资源语言嵌入[71]**：使用低资源语言（如克林贡语）混淆内容；
        - ​**自定义加密**：如密码替换（ciphers）[72]、**CodeChameleon**[79]等。

**补充**：

- Shen等人[90]收集了Reddit、Discord等平台用户分享的真实越狱提示词，验证其对LLM的攻击效果。

###### **b.自动化攻击（Automated Attacks）​**

与依赖专家知识的手工攻击不同，自动化攻击旨在自主发现越狱提示（jailbreak prompts），主要分为两类策略：

1. ​**提示优化（Prompt Optimization）​**
    
    - ​**黑盒方法**：通过优化算法迭代优化提示，提升攻击成功率。
        - ​**AutoDAN**[81]：采用遗传算法搜索对抗提示；
        - ​**GPTFuzzer**[82]：结合突变与生成的模糊测试技术；
        - ​**FuzzLLM**[86]：在自动化模糊测试框架内生成语义连贯的提示；
        - ​**I-FSJ**[93]：向少样本示范（few-shot demonstration）注入特殊标记，并基于示范级随机搜索优化提示，可有效绕过对齐模型及其防御机制。
    - ​**白盒方法**：
        - ​**GCG**[91]：提出贪心坐标梯度算法，搜索对抗后缀（adversarial suffixes）以破解对齐的LLM；
        - ​**IGCG**[92]：改进GCG，引入多样化目标模板与自动多坐标更新策略，攻击成功率接近100%。
2. ​**LLM辅助攻击（LLM-Assisted Attacks）​**
    
    - ​**方法**：利用敌对LLM辅助生成并优化越狱提示。
        - ​**Perez等人[88]**：通过RL微调的LLM生成更有效的对抗提示（但多样性有限）；
        - ​**CRT**[89]：通过最小化**SelfBLEU分数**​（衡量文本多样性）与余弦相似性，提升提示多样性；
        - ​**PAIR**[83]：通过攻击者LLM进行多轮查询，迭代优化越狱提示；
        - ​**ROBOPAIR**[424]：基于PAIR扩展，针对LLM控制的机器人设计提示，引发有害物理行为；
        - ​**ECLIPSE**[95]：利用攻击者LLM自动搜索类似GCG的对抗后缀，实现提示优化流程自动化；
        - ​**Masterkey**[84]：训练敌对LLM以攻击多模型，增强提示迁移性；
        - ​**弱到强越狱（Weak-to-Strong Jailbreaking）​**[94]：通过较弱的非安全模型指导强对齐模型生成有害内容，以极低计算成本实现高成功率。

#### （4）越狱防御

介绍黑盒LLM针对越狱攻击的相应防御机制。基于干预阶段，我们将现有防御分为三类：输入防御、输出防御和集合防御。
###### ​**a.输入防御（Input Defenses）​**

输入防御方法通过对输入提示（prompt）进行预处理，降低其有害内容风险，主要包括以下两类技术：

1. ​**输入重述（Input Rephrasing）​**
    
    - ​**方法**：通过**复述（paraphrasing）​**或**净化（purification）​**模糊提示的恶意意图。
    - ​**案例**：
        - ​**SmoothLLM**[96]：通过随机采样扰动输入提示；
        - ​**SemanticSmooth**[97]：寻找语义相似的安全替代文本；
        - ​**SelfDefend**[98]：执行**词元级扰动（token-level perturbations）​**，移除高困惑度（high perplexity）的对抗性词元；
        - ​**IBProtector**[99]：基于**信息瓶颈原理（information bottleneck principle）​**对编码后的输入进行扰动。
2. ​**输入翻译（Input Translation）​**
    
    - ​**方法**：利用**跨语言转换**​（如回译技术）减轻越狱攻击。
    - ​**案例**：
        - ​**Wang等人[100]**：若目标LLM拒绝原始提示的**回译（back-translated）​**版本，则拒绝响应。其假设是回译过程可揭示提示的潜在恶意意图。
###### **b. 输出防御（Output Defenses）​**

输出防御方法通过监控LLM的生成内容识别有害输出，并在检测到风险时触发**拒绝机制**。主要技术手段如下：

1. ​**输出过滤（Output Filtering）​**
    
    - ​**方法**：对LLM输出内容进行审查，选择性拦截或修改不安全响应。
    - ​**实现方式**：
        - ​**预训练分类器**：如APS[101]、DPP[102]利用安全分类器评估输出风险；
        - ​**内部信号分析**：如**Gradient Cuff**[103]通过分析LLM的**内部拒绝损失函数（internal refusal loss function）​**区分正常查询与恶意请求。
2. ​**输出重复检测（Output Repetition）​**
    
    - ​**原理**：假设LLM对良性请求的响应具有一致性，若无法准确复现输出则可能提示潜在攻击。
    - ​**案例**：
        - ​**PARDEN**[105]：要求LLM重复生成输出，若响应不一致（尤其是针对有害查询时），则判定存在越狱风险。

###### **c.集成防御（Ensemble Defenses）​**

集成防御通过结合多个模型或防御机制，提升系统整体安全性与鲁棒性，其核心思想是不同模型或防御策略的互补性可抵消单一方法的局限性。具体分为两类：

1. ​**多模型集成（Multi-model Ensemble）​**
    
    - ​**方法**：整合多个LLM的推断结果，构建更鲁棒的系统。
    - ​**案例**：
        - ​**MTD**[104]：通过动态调用一组异构LLM，分析多模型输出，选择最安全且相关的响应。
2. ​**多防御集成（Multi-defense Ensemble）​**
    
    - ​**方法**：融合多种防御策略以抵御多样化攻击。
    - ​**案例**：
        - ​**AutoDefense**[106]：结合输入防御与输出防御的集成框架，增强防御效果；
        - ​**MoGU**[107]：基于输入查询，通过**动态路由机制**平衡安全LLM与实用LLM的输出权重，实现重述（rephrasing）与过滤（filtering）的协同防御。

#### （5）提示词注入攻击
​
提示注入攻击通过向正常提示中注入恶意指令，诱导LLM生成非预期输出。本节聚焦**LLM即服务（LLMaaS）​**系统中的黑盒提示注入攻击，分为手工攻击与自动化攻击两类。

---

###### ​**a. 手工攻击（Hand-crafted Attacks）​**

手工攻击依赖专家知识设计注入提示，利用LLM的特定漏洞：

1. ​**直接指令注入**
    - ​**PROMPTINJECT**[108]与**HOUYI**[109]：通过附加恶意指令或构造“忽略上下文”的提示，操控LLM泄露敏感信息。
2. ​**间接数据源注入**
    - ​**Greshake等人[110]**：针对**检索增强型LLM（retrieval-augmented LLMs）​**，通过污染外部数据源注入恶意提示，实现信息窃取、欺诈或内容操控。
3. ​**学术场景攻击**
    - ​**Ye等人[113]**：揭示学术同行评审中LLM的漏洞：
        - ​**显性攻击**：在稿件中嵌入不可见文本，诱导LLM生成虚假正面评审；
        - ​**隐性攻击**：利用LLM对次要缺陷的过度关注，掩盖主要问题。
4. ​**基准与形式化研究**
    - ​**Liu等人[112]**：形式化提示注入攻击与防御，提出组合攻击方法，并建立跨LLM与任务的攻防评估基准。

---

###### ​**b. 自动化攻击（Automated Attacks）​**

自动化攻击通过算法生成并优化恶意提示，突破手工方法局限性：

1. ​**优化框架**
    - ​**Deng等人[111]**：提出LLM驱动的红队框架，迭代生成并优化攻击提示，结合持续安全评估机制。
2. ​**通用攻击生成**
    - ​**Liu等人[114]**：基于梯度方法生成通用提示注入数据，绕过防御机制。
    - ​**G2PIA**[115]：通过最大化干净文本与对抗文本的**KL散度（KL divergence）​**，实现高性价比的提示注入。
3. ​**系统提示窃取**
    - ​**PLeak**[116]：将提示泄露建模为优化问题，构造对抗性查询窃取LLM系统提示。
4. ​**针对特定系统的攻击**
    - ​**JudgeDeceiver**[117]：针对**LLM即裁判（LLM-as-a-Judge）​**系统，通过梯度方法向响应中注入序列，操控LLM偏向攻击者指定输出。
5. ​**对齐过程毒化**
    - ​**PoisonedAlign**[118]：通过毒化LLM的对齐样本，增强模型对提示注入的敏感性，同时保留其核心功能。

#### （6）提示词注入防御

提示注入防御旨在阻止恶意嵌入的指令操控LLM输出，分为**输入防御**与**对抗性微调**两类策略。

---

###### ​**a. 输入防御（Input Defenses）​**

输入防御通过处理输入提示（prompt）消除潜在注入风险，无需修改LLM核心架构：

1. ​**输入重述（Input Rephrasing）​**
    - ​**StuQ**[119]：将用户输入划分为独立的**指令区**与**数据区**，避免指令与数据混淆；
    - ​**SPML**[120]：利用**领域特定语言（Domain-Specific Languages, DSLs）​**定义系统提示，自动分析用户输入是否符合预期，检测恶意请求。

---

###### ​**b. 对抗性微调（Adversarial Fine-tuning）​**

通过模型微调增强LLM区分合法指令与恶意注入的能力：

1. ​**任务限制性微调**
    - ​**Jatmo**[121]：微调目标LLM以限制其仅执行预定义任务，降低对任意指令的敏感性，但牺牲模型的泛化性与灵活性。
2. ​**间接注入防御**
    - ​**Yi等人[122]**提出两种方法：
        - ​**多轮对话（Multi-turn Dialogue）​**：通过轮次隔离外部内容与用户指令；
        - ​**上下文学习（In-context Learning）​**：在提示中添加示例帮助LLM区分数据与指令。
3. ​**偏好优化防御**
    - ​**SecAlign**[123]：将防御建模为**偏好优化（preference optimization）​**问题，构建包含注入提示、安全响应（遵循合法指令）与非安全响应（响应注入）的数据集，优化LLM优先生成安全输出。


#### （7）后门攻击（Backdoor Attacks）​

后门攻击通过**触发注入（trigger injection）​**将后门植入目标模型，主要手段包括**数据投毒**、**训练操控**与**参数修改**。

---

##### ​ 数据投毒（Data Poisoning）

通过污染部分训练数据（注入预定义的后门触发模式），在受污染数据集上训练含后门的模型[425]。

##### ​ 提示级投毒（Prompt-level Poisoning）​

1. ​**离散提示优化（Discrete Prompt Optimization）​**
    
    - ​**方法**：从现有词表中选取离散触发词元（trigger tokens），插入训练数据生成毒化样本。
    - ​**案例**：
        - ​**BadPrompt**[124]：生成与目标标签关联的候选触发词，通过自适应算法选择最有效且隐蔽的触发词；
        - ​**BITE**[125]：迭代识别并注入触发词，建立与目标标签的强关联；
        - ​**ProAttack**[127]：将提示本身作为触发词，实现**干净标签后门攻击（clean-label backdoor）​**。
2. ​**上下文利用（In-Context Exploitation）​**
    
    - ​**方法**：通过篡改输入上下文中的样本或指令注入触发模式。
    - ​**案例**：
        - ​**Instructions as Backdoors**[128]：无需修改数据或标签，通过毒化指令植入后门；
        - ​**Kandpal等人[129]**：验证上下文后门对LLM的可行性，强调需构建跨多样化提示策略的鲁棒后门；
        - ​**ICLAttack**[131]：毒化示范样本与提示，保持模型在干净数据上的精度；
        - ​**ICLPoison**[135]：篡改示范样本破坏上下文学习（in-context learning）。
3. ​**专用提示投毒（Specialized Prompt Poisoning）​**
    
    - ​**方法**：针对特定提示类型或应用场景定制后门。
    - ​**案例**：
        - ​**BadChain**[130]：针对思维链（chain-of-thought）提示，注入后门推理步骤以操控最终响应；
        - ​**PoisonPrompt**[126]：通过双层优化识别硬/软提示的高效触发词，增强上下文推理能力；
        - ​**CODEBREAKER**[137]：利用GPT-4对代码补全模型实施LLM引导的后门攻击，注入隐蔽漏洞；
        - ​**Qiang等人[132]**：毒化指令微调（instruction tuning）阶段，在少量指令数据中注入触发词；
        - ​**Zhang等人[136]**：针对检索增强生成（RAG）模型实施检索投毒；
        - ​**Sleeper Agents**[134]：植入“休眠代理”后门，即使在安全训练后仍保留欺骗性行为。

###### ​多触发投毒（Multi-trigger Poisoning）​

- ​**方法**：通过**多触发词**[43]或**分散触发组件**[138]提升攻击隐蔽性与鲁棒性。
- ​**案例**：
    - ​**CBA**[138]：将触发组件分散至提示各部分，结合提示操控与数据投毒，抵御基础检测方法。

---

##### ​ 训练操控（Training Manipulation）​

- ​**方法**：直接操控训练过程（如优化目标或梯度）注入后门。
- ​**案例**：
    - ​**Gu等人[139]**：将后门注入视为**多任务学习（multi-task learning）​**，通过控制梯度幅度与方向防止后门遗忘；
    - ​**TrojLLM**[140]：在黑盒环境下生成通用隐蔽触发词，基于渐进式特洛伊投毒算法；
    - ​**VPI**[141]：针对指令微调模型，使模型在特定触发下响应用户指令时附加攻击者预设的虚拟提示；
    - ​**Yang等人[142]**：篡改训练中LLM的**不确定性校准（uncertainty calibration）​**机制，利用置信度估计植入后门。

---

##### ​ 参数修改（Parameter Modification）​

- ​**方法**：直接修改模型参数（如定位特定神经元）植入后门。
- ​**案例**：
    - ​**BadEdit**[143]：将后门注入视为**轻量级知识编辑（knowledge-editing）​**问题，以极少数据高效修改LLM参数。

#### （8）后门防御（Backdoor Defenses）​

针对LLM的后门防御方法可分为四类：​**后门检测**、**后门移除**、**鲁棒训练**与**鲁棒推断**。

---

##### ​ 后门检测（Backdoor Detection）​

后门检测旨在识别被篡改的输入或模型，提前标记潜在威胁。现有方法假设可访问被植入后门的模型，但无法获取原始训练数据或攻击细节，主要聚焦于检测触发后门行为的输入：

- ​**IMBERT**[144]：利用梯度与自注意力分数识别导致异常预测的关键词元（tokens）；
- ​**AttDef**[145]：通过归因分数（attribution scores）定位对错误预测影响显著的触发词；
- ​**SCA**[146]：微调模型降低对触发词的敏感性，确保语义一致性；
- ​**ParaFuzz**[147]：对输入进行复述并对比预测结果，检测触发模式的不一致性；
- ​**MDP**[148]：识别关键后门模块，在微调时冻结相关参数以抑制其影响。  
    **局限**：对复杂多触发攻击的检测能力有限。

---

##### ​ 后门移除（Backdoor Removal）​

通过修改模型参数消除或抑制后门行为，分为两类策略：

1. ​**剪枝方法（Pruning Methods）​**
    
    - ​**原理**：移除与后门强相关的模型组件，保留干净输入的预测性能。
    - ​**案例**：
        - ​**PCP Ablation**[149]：替换后门激活关键模块为低秩近似，中和后门影响。
2. ​**微调方法（Fine-tuning Methods）​**
    
    - ​**原理**：在干净数据上重新训练模型，弱化触发词与目标输出的关联。
    - ​**案例**：
        - ​**SANDE**[150]：基于良性输入-输出对直接覆盖后门映射；
        - ​**CROW**[152]与**BEEAR**[151]：分别通过增强内部一致性或对抗嵌入漂移消除后门。

---

##### ​ 鲁棒训练（Robust Training）​

在训练过程中引入机制抑制后门映射或增强模型鲁棒性：

- ​**Honeypot Defense**[153]：训练时引入专用模块隔离后门特征；
- ​**Liu等人[154]**：通过最大化熵损失（maximum entropy loss）对抗后门攻击的最小化交叉熵策略；
- ​**Wang等人[155]**：去除重复触发元素并抑制后门相关记忆机制。

---

##### ​鲁棒推断（Robust Inference）​

通过调整推断过程减少后门对生成结果的影响：

- ​**对比解码（Contrastive Decoding）​**：对比潜在后门模型与干净参考模型的输出，修正恶意内容。
    - ​**PoisonShare**[156]：利用多轮对话中的中间层表示指导对比解码，检测并修正被投毒语句；
    - ​**CleanGen**[157]：用干净参考模型的预测替换可疑词元，最小化后门效应。  
        **局限**：依赖可信的干净参考模型，实际场景中可能难以获取。


#### ​（9） 安全对齐（Safety Alignment）​

大型语言模型（LLMs）的卓越能力带来了独特的**对齐挑战**：如何确保模型与人类价值观一致，避免生成有害内容、传播错误信息或延续偏见。对齐的核心目标是弥合LLM预训练期间学习的统计模式与人类社会复杂、微妙的期望之间的差距。本节梳理现有对齐（及安全对齐）研究，将其归纳为三类：​**基于人类反馈的对齐（RLHF）​**、**基于AI反馈的对齐（RLAIF）​**和**基于社交互动的对齐**。

---

##### ​**a. 基于人类反馈的对齐（Alignment with Human Feedback）​**

直接通过人类偏好指导模型行为对齐，主要方法包括：

1. ​**近端策略优化（Proximal Policy Optimization, PPO）​**
    
    - ​**原理**：将人类反馈作为奖励信号微调LLM，通过最大化人类评估的预期奖励对齐模型输出。
    - ​**案例**：
        - ​**InstructGPT**[160]：验证PPO在指令遵循与高质量生成中的有效性；
        - ​**Safe-RLHF**[161]：添加安全约束，确保输出在有益性最大化的同时不越界。
    - ​**局限**：对超参数敏感且训练稳定性较低。
2. ​**直接偏好优化（Direct Preference Optimization, DPO）​**
    
    - ​**原理**：直接利用人类偏好数据优化LLM，无需独立奖励模型。
    - ​**案例**：
        - ​**标准DPO**[162][163]：通过预测偏好分数排序响应；
        - ​**MODPO**[164]：扩展至多目标优化，平衡有益性、无害性、真实性等多维度偏好。
3. ​**卡尼曼-特沃斯基优化（Kahneman-Tversky Optimization, KTO）​**
    
    - ​**原理**：基于前景理论（Prospect Theory），通过不对称损失函数（重罚不可取结果，轻奖可取结果）引导模型远离不良输出。
    - ​**优势**：降低对直接人类监督的依赖，适用于难以明确定义可取结果的场景。
4. ​**监督微调（Supervised Fine-Tuning, SFT）​**
    
    - ​**原理**：基于高质量标注数据集微调模型，强调数据质量而非数量。
    - ​**案例**：
        - ​**LIMA**[166]：少量精选数据即可对齐强大预训练模型，验证格式与风格聚焦的有效性。
    - ​**局限**：数据标注耗时且需领域专业知识。

---

##### ​**b.基于AI反馈的对齐（Alignment with AI Feedback）​**

通过AI生成反馈扩展对齐规模，减少人类参与：

1. ​**近端策略优化（RLAIF-PPO）​**
    - ​**案例**：
        - ​**宪法AI（CAI）​**[167]：基于预设原则的AI自洽性评估与PPO优化；
        - ​**SELF-ALIGN**[168]：结合原则驱动推理与LLM生成能力，通过PPO迭代修正；
        - ​**RLCD**[169]：生成多样化偏好对训练偏好模型，指导PPO微调。

---

##### ​**c.基于社交互动的对齐（Alignment with Social Interactions）​**

通过模拟社交环境训练LLM遵循社会规范：

1. ​**对比性策略优化（Contrastive Policy Optimization, CPO）​**
    - ​**案例**：
        - ​**Stable Alignment**[170]：在基于规则的模拟社会中训练模型，观察行为后果以学习社会规范；
        - ​**MATRIX**[171]：通过**多角色自对话模拟（Monopolylogue）​**生成社交场景，激活LLM内在社会知识，无需外部监督即可实现强对齐。

#### （10）能耗-延迟攻击（Energy-Latency Attacks, ELAs）​

能耗-延迟攻击（ELAs）旨在通过增加LLM推断的计算需求，降低模型推断效率，导致**推断延迟上升**与**能耗增加**。现有ELAs可分为以下两类：

---

##### ​a.白盒攻击（White-box Attacks）​

假设攻击者完全掌握模型参数，可精确操控推断过程，分为：

1. ​**基于梯度的攻击（Gradient-based Attacks）​**
    - ​**原理**：利用梯度信息识别最大化计算量的输入扰动，破坏高效推断机制（如**句子结束预测（EOS prediction）​**、**早退机制（early-exit）​**）。
    - ​**案例**：
        - ​**NMTSloth**[172]：针对神经机器翻译模型的EOS预测机制；
        - ​**SAME**[173]：干扰多出口模型的早退机制；
        - ​**LLMEffiChecker**[174]：针对多种LLM的梯度攻击框架；
        - ​**TTSlow**[175]：诱导文本到语音系统生成无限循环语音。
    - ​**局限**：计算成本高、模型特异性强，泛化能力有限。

---

##### ​b.黑盒攻击（Black-box Attacks）​

仅需输入-输出接口，通过构造输入样本诱导推断延迟：

1. ​**基于查询的攻击（Query-based Attacks）​**
    
    - ​**原理**：通过重复查询构造对抗样本，利用特定模型行为增加计算量。
    - ​**案例**：
        - ​**No-Skim**[176]：针对基于略读（skimming）的模型，扰动输入以最大化保留词元数量。
    - ​**局限**：对非略读模型无效，实际场景中耗时较长。
2. ​**基于投毒的攻击（Poisoning-based Attacks）​**
    
    - ​**案例**：
        - ​**P-DoS**[177]：在微调阶段注入单个毒化样本，诱导模型生成超长输出，突破输出长度限制并增加延迟（即使通过有限API访问）。

---

##### ​**当前挑战与未来方向**

ELAs对LLM构成新兴威胁，但现有研究存在以下局限性：

- ​**攻击策略**：多数方法依赖特定架构、计算成本高或黑盒场景效果有限；
- ​**防御机制**：现有防御（如运行时输入验证）可能引入额外开销。

未来研究可聚焦：

1. 开发跨LLM架构与部署场景的**通用高效攻击与防御方法**；
2. 探索更隐蔽的攻击向量（如多阶段投毒）；
3. 优化防御机制的计算效率与兼容性。


#### （11）模型提取攻击（Model Extraction Attacks, MEAs）​

模型提取攻击（MEAs），亦称**模型窃取攻击**，对LLM的安全性及知识产权构成重大威胁。其目标是通过策略性查询与分析目标LLM的响应，构建功能相似的替代模型。现有针对LLM的MEAs可分为两类：​**微调阶段攻击**与**对齐阶段攻击**。

---

##### ​a.微调阶段攻击（Fine-tuning Stage Attacks）​

旨在从针对下游任务微调的LLM中提取知识，分为两类：

1. ​**功能相似性提取（Functional Similarity Extraction）​**
    
    - ​**方法**：以目标模型的输入-输出行为为引导，​**蒸馏（distill）​**其学习到的知识。
    - ​**案例**：
        - ​**LION**[178]：利用目标模型作为裁判与生成器，迭代提升学生模型的指令遵循能力。
2. ​**特定能力提取（Specific Ability Extraction）​**
    
    - ​**方法**：通过针对性查询提取目标模型习得的特定技能或知识。
    - ​**案例**：
        - ​**Li等人[179]**：通过精心构造的查询从黑盒LLM API中提取代码生成能力。
    - ​**局限**：替代模型依赖目标模型的泛化能力，对未见输入效果有限。

---

##### ​b. 对齐阶段攻击（Alignment Stage Attacks）​

旨在提取目标LLM的对齐属性（如安全性、有益性），具体目标是窃取指导这些属性的**奖励模型（reward model）​**。

- ​**功能相似性提取**：复现目标模型的对齐偏好。
    - ​**案例**：
        - ​**LoRD**[180]：基于策略梯度方法，同时提取任务特定知识与对齐属性。
    - ​**挑战**：精确捕捉人类偏好的复杂性仍具挑战性。

---

##### ​**总结与展望**

MEAs是快速演进的LLM安全威胁。当前攻击虽能成功提取任务知识及对齐属性，但仍面临**精确复现目标模型完整功能**的难题。未来需开发主动防御策略，保护LLM免受模型提取攻击。


#### （12） 数据提取攻击（Data Extraction Attacks）​

大型语言模型（LLMs）可能记忆训练数据中的部分内容，导致**隐私泄露风险**。数据提取攻击通过恢复训练样本，可能暴露敏感信息（如个人身份信息PII、版权内容或机密数据）。本节梳理针对LLM的数据提取攻击，涵盖白盒与黑盒攻击两类。

---

##### ​**a. 白盒攻击（White-box Attacks）​**

白盒攻击聚焦于**潜在记忆提取（Latent Memorization Extraction）​**，即从模型参数或激活状态中提取隐含信息（无法通过输入-输出接口直接获取）：

- ​**Duan等人[191]**：通过分析内部表示提取潜在数据，如向权重添加噪声或检查交叉熵损失，在Pythia-1B、Amber-7B等模型上验证。
- ​**局限**：需完全访问模型参数，实际攻击场景受限。

---

##### ​**b. 黑盒攻击（Black-box Attacks）​**

黑盒攻击通过构造诱导性提示词诱使LLM泄露记忆数据，无需访问模型参数，分为以下类型：

1. ​**前缀攻击（Prefix Attacks）​**
    
    - ​**原理**：利用LLM的自回归特性，提供记忆序列的“前缀”诱导模型补全后续内容。
    - ​**案例**：
        - ​**Carlini等人[181]**：在GPT-2等模型上验证；
        - ​**Nasr等人[183]**：利用后缀数组扩展攻击规模；
        - ​**Magpie**[184]与**Al-Kaswan等人[185]**：针对PII或代码数据定向提取；
        - ​**Yu等人[190]**：通过多样化采样策略（Top-k、Nucleus）、概率调整（温度系数、重复惩罚）、动态上下文窗口、前瞻机制及改进后缀排序（Zlib压缩、高置信度词元）提升攻击效率。
2. ​**特殊字符攻击（Special Character Attack）​**
    
    - ​**原理**：利用模型对特殊字符或异常格式的敏感性触发意外行为。
    - ​**案例**：
        - ​**SCA**[186]：特定字符可诱导LLM泄露训练数据。
    - ​**防御**：通过输入清洗（sanitization）缓解此类漏洞。
3. ​**提示优化（Prompt Optimization）​**
    
    - ​**方法**：利用“攻击者LLM”生成优化提示词，自动化诱导“受害者LLM”泄露数据。
    - ​**案例**：
        - ​**Kassem等人[187]**：结合迭代拒绝采样与**最长公共子序列（LCS）​**优化提示生成。
    - ​**局限**：依赖攻击者LLM能力，计算成本较高。
4. ​**检索增强生成（RAG）提取攻击**
    
    - ​**目标**：从RAG系统的检索组件中泄露敏感信息。
    - ​**案例**：
        - ​**Qi等人[188]**：通过对抗提示触发RAG系统数据泄露，揭示LLM与外部知识库集成的安全风险。
5. ​**集成攻击（Ensemble Attack）​**
    
    - ​**方法**：结合多种攻击策略（如前缀攻击+提示优化）提升成功率。
    - ​**案例**：
        - ​**More等人[189]**：在Pythia模型上验证集成攻击有效性。
    - ​**挑战**：需协调各攻击组件，设计复杂度高。

#### （13）数据集与基准

表5总结了LLM安全研究中常用的数据集与基准，按评估目标分为四类：​**毒性数据集**、**真实性数据集**、**价值观基准**、**对抗性数据集与后门基准**。

---

##### ​**a. 毒性数据集（Toxicity Datasets）​**

旨在评估LLM生成有害内容的倾向：

- ​**RealToxicityPrompts**[426]：包含10万条提示及其通过Perspective API计算的毒性分数，揭示预训练数据毒性与模型输出的强相关性，但依赖Perspective API可能引入偏见。
- ​**Do-NotAnswer**[430]：包含939条设计用于诱导有害响应的提示，涵盖错误信息、歧视等风险类别，需人工评估模型安全性，成本较高。
- ​**最新方法**[441]：提出众包毒性问答数据集，结合人工与LLM标注，采用**双层优化框架（bilevel optimization）​**与**GroupDRO**提升对分布外风险的鲁棒性，减少人工标注需求。

---

##### ​**b. 真实性数据集（Truthfulness Datasets）​**

评估LLM生成信息的真实性：

- ​**TruthfulQA**[427]：包含38个类别、817个问题，聚焦“模仿性虚假”（从人类文本中习得的错误答案），发现大模型存在**逆向缩放（inverse scaling）​**现象（模型越大真实性越低）。

---

##### ​**c. 价值观基准（Value Benchmarks）​**

评估LLM与人类价值观的对齐性：

- ​**FLAMES**[433]：通过2,251条提示评估中文LLM在公平性、安全性、道德性等维度的对齐表现；
- ​**SORRY-Bench**[434]：评估LLM拒绝不安全请求的能力，覆盖45个主题类别；
- ​**CVALUES**[431]：兼顾安全性与责任性评估；
- ​**SafetyPrompts**[429]：针对中文LLM的伦理场景评估基准；
- ​**FINE框架**[432]：针对**虚假对齐（fake alignment）​**问题（模型仅记忆安全答案），提出一致性评估方法；
- ​**SafetyBench**[435]：提供高效自动化的多选题基准，支持快速评估；
- ​**Libra-Leaderboard**[440]：平衡模型安全性与能力的评估榜单，包含57个数据集、对抗测试交互平台及统一评分系统。

---

##### ​**d.对抗性数据集与后门基准**

评估LLM对对抗攻击与后门攻击的鲁棒性：

- ​**BackdoorLLM**[437]：首个文本生成后门攻击基准，支持数据投毒、权重投毒等多种攻击策略；
- ​**Adversarial GLUE**[428]：使用14种文本攻击方法评估LLM鲁棒性；
- ​**SALAD-Bench**[436]：扩展安全基准，涵盖攻击增强与防御增强问题分类；
- ​**JailBreakV-28K**[438]：针对多模态LLM的越狱攻击测试集（文本与图像用例）；
- ​**A STRONGREJECT**[439]：通过高质量数据集与自动化评估改进越狱检测。  
    **挑战**：现有基准在可扩展性、一致性与现实相关性方面仍需提升。


## 三.视觉语言预训练模型（VLP）

视觉语言预训练（VLP）模型（如CLIP[442]、ALBEF[443]、TCL[444]）在**对齐视觉与文本模态**方面取得了显著进展，但仍易受多种安全威胁，引发越来越多的研究关注。本节系统梳理当前VLP模型的安全研究进展，聚焦**对抗攻击**、**后门攻击**与**投毒攻击**三类威胁，相关代表性方法总结于表6。

### **(1) 对抗攻击**

视觉语言预训练（VLP）模型常作为下游任务的骨干网络进行微调，针对VLP的对抗攻击旨在生成可导致零样本图像分类、图文检索、视觉蕴含（visual entailment）、视觉定位（visual grounding）等任务错误预测的对抗样本。根据威胁模型，此类攻击可分为**白盒攻击**与**黑盒攻击**。

---

#### ​a. 白盒攻击

根据扰动类型，白盒攻击进一步分为**不可见扰动**与**可见扰动**，现有攻击主要采用前者：

1. ​**不可见扰动（Invisible Perturbations）​**
    
    - ​**方法**：对输入（文本或图像）施加微小且不可察觉的扰动，保持攻击隐蔽性。
    - ​**早期研究**：视觉与语言领域独立开发不可见扰动技术（如[60][445]-[447]）。
    - ​**VLP场景**：
        - ​**Co-Attack**[192]：首次提出**同时扰动视觉与文本输入**以增强攻击效果；
        - ​**AdvCLIP**[193]：探索可欺骗所有下游任务的**通用对抗扰动（UAPs）​**。
2. ​**可见扰动（Visible Perturbations）​**
    
    - ​**方法**：通过显著且可感知的输入篡改实现攻击。
    - ​**案例**：
        - ​**手工构造的排版、概念与符号图像**可诱导CLIP模型“先读后看”（优先依赖文本而非视觉信息），揭示VLP模型的独特行为模式，为攻击设计提供新思路[194]。

---

#### ​b.黑盒攻击

黑盒攻击主要采用**基于迁移的方法**，而基于查询的攻击研究较少。现有方法分为两类：

1. ​**样本级扰动（Sample-wise Perturbations）​**
    
    - ​**特点**：攻击效果更优，但跨任务迁移性有限。
    - ​**案例**：
        - ​**SGA**[195]：利用跨模态交互与对齐保留增强策略探索对抗迁移性；
        - ​**SA-Attack**[196]：向原始输入与对抗输入引入数据增强，提升跨模态迁移性；
        - ​**VLP-Attack**[197]：通过**对比损失函数（contrastive loss）​**生成对抗文本与图像；
        - ​**TMM**[198]：结合基于注意力的模态一致性扰动与正交引导的模态差异性扰动；
        - ​**VLATTACK**[199]：在单模态与多模态层级融合图像与文本扰动；
        - ​**PRM**[200]：基于CLIP等基础模型针对下游任务（如目标检测、图像描述）设计可迁移攻击。
2. ​**通用扰动（Universal Perturbations）​**
    
    - ​**特点**：攻击效果较弱但迁移性更强。
    - ​**案例**：
        - ​**C-PGC**[201]：首个针对VLP模型的通用对抗扰动方法，利用对比学习与跨模态信息破坏图文嵌入对齐；
        - ​**ETU**[202]：生成可跨VLP模型与任务迁移的通用扰动，通过改进全局与局部优化技术提升效果，并引入**ScMix数据增强策略**​（融合自混合与跨混合操作）提升扰动鲁棒性与适用性。

###  **(2)对抗防御**

针对VLP模型的对抗防御方法可分为四类：​**对抗样本检测**、**标准对抗训练**、**对抗提示调优**与**对抗对比调优**。前两者通过检测或训练增强鲁棒性，后两者通过轻量调优优化模型。

---

#### ​**a.对抗样本检测（Adversarial Example Detection）**

1. ​**一次性检测（One-shot Detection）​**
    
    - ​**原理**：单次前向传播即可区分对抗样本与干净样本。
    - ​**案例**：
        - ​**MirrorCheck**[217]：模型无关方法，利用**文本到图像（T2I）模型**生成图像，通过CLIP图像编码器比较输入图像与生成图像的相似度差异，检测对抗样本。
2. ​**状态检测（Stateful Detection）​**
    
    - ​**场景**：针对黑盒查询攻击，需追踪多次查询行为。
    - ​**案例**：
        - ​**AdvQDet**[218]：通过**对抗对比提示调优（ACPT）​**调整CLIP图像编码器，仅需三次查询即可检测对抗性查询。

---

#### ​**b.标准对抗训练（Standard Adversarial Training）​**

对抗训练被广泛认为是对抗攻击最有效的防御手段[409][448]，但对VLP模型的高计算成本构成挑战。代表性方法：

1. ​**VILLA**[216]：两阶段视觉语言对抗训练框架
    
    - ​**阶段1**：任务无关对抗预训练（嵌入空间扰动，非像素/词元级）；
    - ​**阶段2**：任务特定微调，采用**FreeLB策略**[449]降低计算开销。
2. ​**AdvXL**[215]：大规模对抗训练框架
    
    - ​**粗调阶段**：低分辨率图像+弱攻击轻量预训练；
    - ​**精调阶段**：全分辨率图像+强攻击密集微调。

---

#### ​**c.对抗提示调优（Adversarial Prompt Tuning）​**

在提示调优中融入对抗训练，轻量化提升VLP模型鲁棒性：

1. ​**文本提示调优（Textual Prompt Tuning）​**
    
    - ​**AdvPT**[204]：通过可学习文本提示对齐对抗图像嵌入与干净文本嵌入；
    - ​**APT**[205]：学习鲁棒文本提示，提升CLIP图像编码器精度与鲁棒性；
    - ​**MixPrompt**[206]：结合条件对抗提示调优，增强模型泛化性与鲁棒性；
    - ​**Defense-Prefix**[203]：添加前缀词元抵御排版攻击，无需微调。
2. ​**多模态提示调优（Multi-Modal Prompt Tuning）​**
    
    - ​**FAP**[208]：引入可学习对抗文本监督，平衡跨模态一致性；
    - ​**APD**[209]：通过教师-学生多模态提示蒸馏提升CLIP鲁棒性；
    - ​**TAPT**[210]：测试时防御方法，学习防御性双模态提示提升零样本推断鲁棒性。

---

#### ​**d.对抗对比调优（Adversarial Contrastive Tuning）​**

通过对比学习与对抗训练微调CLIP图像编码器，分为监督与无监督方法：

1. ​**监督对比调优**
    
    - ​**视觉调优**：仅用对抗图像微调。
        - ​**TeCoA**[211]：发现无文本引导时视觉提示调优更有效；
        - ​**PMG-AFT**[212]：引入辅助分支最小化对抗输出与预训练模型的距离，防止过拟合。
    - ​**多模态调优**：结合对抗文本与图像微调。
        - ​**MMCoA**[213]：融合图像PGD攻击与文本BERT-Attack，通过双对比损失对齐干净/对抗特征。
2. ​**无监督对比调优**
    
    - ​**FARE**[214]：无监督对抗微调CLIP图像编码器，提升零样本分类与视觉语言任务鲁棒性，支持LLaVA、OpenFlamingo等模型无需重新训练。

### **(3)后门攻击与投毒攻击**

针对CLIP模型的后门与投毒攻击可针对**预训练阶段**或**下游任务微调阶段**。研究表明，CLIP的后门投毒攻击所需投毒率远低于传统监督学习[224]，且基于网络爬取数据训练的CLIP更易受后门攻击[453]。本节梳理针对CLIP的后门与投毒攻击方法。

---

#### ​**a. 后门攻击**

根据触发模态，后门攻击可分为**视觉触发器**与**多模态触发器**两类：

1. ​**视觉触发器（Visual Triggers）​**
    
    - ​**目标**：在预训练图像编码器中植入后门模式。
    - ​**案例**：
        - ​**BadEncoder**[219]：在自监督学习（self-supervised learning）中通过污染预训练图像编码器植入后门，影响下游分类器；
        - ​**CorruptEncoder**[220]：利用对比学习中的随机裁剪（random cropping）注入后门（裁剪视图仅包含参考对象或触发器时效果更佳）；
        - ​**BadCLIP**[221]：通过**双嵌入引导（dual-embedding guidance）​**优化视觉触发模式，使其与目标文本及特定视觉特征对齐，绕过后门检测与微调防御。
2. ​**多模态触发器（Multi-modal Triggers）​**
    
    - ​**方法**：结合视觉与文本触发器增强攻击效果。
    - ​**案例**：
        - ​**BadCLIP**[222]：在提示学习（prompt learning）阶段注入可学习触发器（仅影响图像与文本编码器，无需微调整个模型）。

---

#### ​**b.投毒攻击**

针对CLIP的定向投毒攻击包括**PBCL**[224]与**MM Poison**[223]：

- ​**PBCL**：验证仅需投毒**0.0001%训练数据**即可实现特定样本的定向误分类；
- ​**MM Poison**：利用多模态漏洞设计三类攻击：
    1. ​**单目标图像攻击**：污染单一目标图像；
    2. ​**单目标标签攻击**：污染单一目标标签；
    3. ​**多目标标签攻击**：污染多个目标标签。  
        ​**效果**：在保持干净数据性能的同时，视觉与文本模态均实现高攻击成功率。

### **(4) 后门与投毒防御**

针对后门与投毒攻击的防御策略分为**鲁棒训练（Robust Training）​**与**后门检测（Backdoor Detection）​**两类。前者旨在训练对攻击具有抗性的VLP模型，后者专注于识别被篡改的编码器或污染数据，通常需结合缓解技术彻底消除后门效应。

---

#### ​**a. 鲁棒训练**

根据模型获得鲁棒性的阶段，鲁棒训练策略分为**微调阶段**与**预训练阶段**方法：

##### ​**微调阶段**

- ​**CleanCLIP**[225]：通过**重新对齐各模态的表征**​（如视觉与文本），削弱后门攻击引入的虚假关联；
- ​**SAFECLIP**[226]：利用**单模态对比学习**增强特征对齐，通过高斯混合模型将数据分为安全集与风险集，预训练时优化安全集的CLIP损失，并单独微调风险集，降低毒化图文对的相似性以防御定向投毒与后门攻击。

##### ​**预训练阶段**

- ​**ROCLIP**[227]：通过在预训练阶段增强模型鲁棒性来防御投毒与后门攻击，具体策略包括：
    - 利用大规模多样化随机文本描述池破坏毒化图文对的关联；
    - 应用图像与文本增强技术强化防御并提升模型性能。

---

#### ​**b.后门检测**

后门检测可细分为三个子任务：​**触发器逆向工程（Trigger Inversion）​**、**后门样本检测（Backdoor Sample Detection）​**和**后门模型检测（Backdoor Model Detection）​**。其中，触发器逆向工程是核心，因其恢复的触发模式可用于检测后门样本与模型。

1. ​**触发器逆向工程**
    
    - ​**Mudjacking**[230]：检测到含触发器的误分类输入时，通过调整模型参数移除后门；
    - ​**TIJO**[229]：针对双模态后门攻击，联合优化图像与文本模态的逆向触发模式。
2. ​**后门样本检测**
    
    - ​**SEER**[231]：在共享特征空间中联合检测恶意图像触发器与目标文本，无需访问训练数据或下游任务知识；
    - ​**Outlier Detection**[232]：利用后门样本的局部邻域稀疏性（相比干净样本），应用局部异常检测方法高效识别网络规模数据集中的后门样本。实验发现，开源CLIP编码器中已存在**CC3M数据集**潜在的无意识后门样本。
3. ​**后门模型检测**
    
    - ​**DECREE**[228]：针对VLP编码器的无标签检测方法，利用后门模型在干净与后门输入下的嵌入空间特性差异，结合触发器逆向工程识别被篡改的编码器。
### **(5) 数据集**

表6总结了用于VLP安全研究的基准数据集，涵盖对抗攻击与防御的评估任务。具体包括以下类型：

1. ​**图像分类任务**：
    
    - ​**常用数据集**：ImageNet [454]、Caltech101 [455]、DTD [456]、EuroSAT [457]、OxfordPets [458]、FGVC-Aircraft [459]、Food101 [460]、Flowers102 [461]、StanfordCars [462]、SUN397 [463]、UCF101 [464]。
    - ​**领域泛化与分布变化鲁棒性评估**：ImageNetV2 [465]、ImageNet-Sketch [466]、ImageNetA [467]、ImageNet-R [468]。
2. ​**其他任务**：
    
    - ​**图文检索**：MS-COCO [418]、Flickr30K [469]；
    - ​**视觉定位（Visual Grounding）​**：RefCOCO+ [470]；
    - ​**视觉蕴含（Visual Entailment）​**：SNLI-VE [471]。
## ​四. 视觉语言模型（VLM）

大型视觉语言模型（VLMs）通过引入**视觉模态**扩展了大型语言模型（LLMs）的能力，其核心组件包括预训练的图像编码器（pre-trained image encoders）与对齐模块（alignment modules），从而支持视觉对话、复杂推理等应用。然而，这种多模态架构也引入了独特的安全漏洞。本节系统梳理针对VLMs的**对抗攻击**、**延迟能耗攻击**、**越狱攻击**、**提示注入攻击**、**后门与投毒攻击**及其防御方法。

#### ​**攻击与防御的通用性**

许多VLMs基于视觉语言预训练（Vision-Language Pretraining, VLP）的编码器构建，因此第4章讨论的攻击与防御方法同样适用于VLM。然而，VLM的预训练编码器与LLM之间的**额外对齐过程**扩大了攻击面，引入了新的风险，例如：

1. ​**跨模态后门攻击**：通过污染视觉与文本模态的关联性触发恶意行为；
2. ​**多模态越狱攻击**：通过组合文本与图像输入绕过安全机制。

#### ​**安全挑战与应对**

这些风险突显了针对VLMs定制化安全措施的必要性。未来研究需重点关注：

- ​**跨模态攻击检测与防御**：开发能够识别并阻断多模态协同攻击的机制；
- ​**对齐过程的安全性验证**：确保视觉与文本模态对齐不引入隐蔽漏洞；
- ​**高效鲁棒训练框架**：在保持多模态能力的同时增强模型抗攻击能力。

### ​（1） 对抗攻击

视觉语言模型（VLMs）的对抗攻击主要针对**视觉模态**。与文本不同，图像的高维特性使其更易受对抗性扰动的影响。攻击者通过向图像添加难以察觉的扰动，旨在破坏图像描述生成（image captioning）、视觉问答（VQA）等任务。根据威胁模型，此类攻击可分为**白盒攻击**、**灰盒攻击**与**黑盒攻击**。

---

#### ​**a. 白盒攻击**

白盒攻击假设攻击者完全掌握VLM参数（包括视觉编码器与大语言模型），根据攻击目标可分为三类：

1. ​**任务特定攻击（Task-specific Attacks）​**
    
    - ​**Schlarmann等人[233]**：首次揭示Flamingo[472]、GPT-4[473]等VLM对对抗图像的脆弱性，证明攻击者可通过篡改图像描述误导用户（如跳转恶意网站或传播虚假信息）。
    - ​**Gao等人[236]**：针对指代表达理解任务（referring expression comprehension）设计攻击范式；
    - ​**[234]**：提出查询分解方法，证明上下文提示（contextual prompts）可增强VLM对视觉攻击的鲁棒性。
2. ​**跨提示攻击（Cross-prompt Attack）​**
    
    - 目标：确保对抗图像在不同提示下均有效。
    - ​**CroPA[235]**：探索单个对抗图像在多种提示下的迁移性，提出通过可学习提示优化扰动以提升迁移性。
3. ​**思维链攻击（Chain-of-Thought, CoT Attack）​**
    
    - ​**Stop-reasoning Attack[237]**：研究CoT推理对VLM对抗鲁棒性的影响，设计新型攻击绕过防御并干扰模型推理过程。

---

#### ​**b.灰盒攻击**

灰盒攻击通常仅访问VLM的**视觉编码器**或**大语言模型**​（以视觉编码器为主）。攻击者通过生成与目标图像相似的对抗样本操控模型预测，无需完全访问VLM参数。

- ​**InstructTA[238]**：生成目标图像，利用代理模型创建对抗性扰动，最小化原图与对抗图的特征距离，并结合GPT-4改写指令提升迁移性。

---

#### ​**c.黑盒攻击**

黑盒攻击无需访问目标模型参数，主要依赖**基于迁移**或**基于生成器**的方法：

1. ​**基于迁移的攻击（Transfer-based Attacks）​**
    
    - 利用冻结的CLIP视觉编码器（广泛用于VLMs）的通用性。
        - ​**AttackBard[239]**：证明代理模型生成的对抗图像可绕过Google Bard的防御机制；
        - ​**AttackVLM[240]**：针对CLIP[442]、BLIP[474]等模型生成定向对抗图像，验证跨VLM迁移攻击的有效性。
2. ​**基于生成器的攻击（Generator-based Attacks）​**
    
    - 利用生成模型（如扩散模型）提升对抗样本的迁移性。
        - ​**AdvDiffVLM[241]**：结合自适应集成梯度估计与GradCAM引导掩码，生成语义嵌入更自然的对抗图像；
        - ​**AnyAttack[242]**：自监督框架，通过对比损失生成无标签监督的定向对抗样本，误导多任务模型。

### ​（2）越狱攻击（Jailbreak Attacks）​

视觉语言模型（VLMs）的视觉模态为越狱攻击提供了额外途径。与对抗攻击（引发随机或定向错误）不同，越狱攻击专门针对模型的安全防护机制，旨在生成不当输出。根据威胁模型，VLM越狱攻击可分为**白盒攻击**与**黑盒攻击**。

---

#### ​**a.白盒攻击**

白盒越狱攻击利用梯度信息扰动输入图像或文本，攻击VLM的特定安全机制，分为三类：

1. ​**目标特定越狱（Target-specific Jailbreak）​**
    
    - ​**Image Hijack**[243]：通过对抗图像操控VLM输出，如泄露信息、绕过安全措施、生成虚假陈述。
    - ​**Adversarial Alignment Attack**[244]：证明对抗图像可诱导VLM行为失准（misalignment），类似技术可适配至纯文本模型。
2. ​**通用越狱（Universal Jailbreak）​**
    
    - ​**VAJM**[245]：单张对抗图像即可绕过VLM安全机制，强制生成通用有害输出。
    - ​**ImgJP**[246]：利用最大似然算法生成可迁移对抗图像，越狱多种VLM，甚至通过图像转文本提示桥接VLM与LLM攻击。
3. ​**混合越狱（Hybrid Jailbreak）​**
    
    - ​**UMK**[247]：双模态优化攻击，在图像与文本中嵌入恶意语义以最大化影响。
    - ​**HADES**[248]：结合通用对抗图像与精心设计的输入绕过安全机制，放大有害指令的对抗操控能力。

---

#### ​**b.黑盒攻击**

黑盒越狱攻击无需访问VLM内部参数，利用外部漏洞（如冻结的CLIP视觉编码器、跨模态交互缺陷、系统提示泄漏）绕过防御，分为四类：

1. ​**基于迁移的攻击（Transfer-based Attacks）​**
    
    - ​**Jailbreak in Pieces**[249]：利用开源图像编码器生成对抗图像，结合干净文本提示破坏VLM对齐性。
2. ​**手动设计攻击（Manually-designed Attacks）​**
    
    - ​**FigStep**[250]：通过排版将有害文本转为图像，利用VLM视觉理解能力绕过安全机制。
    - ​**VRP**[252]：视觉角色扮演方法，基于LLM生成高风险角色图像，结合无害指令利用角色负面特征诱导有害输出。
3. ​**系统提示泄漏（System Prompt Leakage）​**
    
    - ​**SASP**[251]：利用GPT-4V的系统提示泄漏漏洞，使模型执行自对抗攻击，揭示内部提示暴露风险。
4. ​**红队测试（Red Teaming）​**
    
    - ​**IDEATOR**[253]：集成VLM与先进扩散模型，自动生成恶意图文对，克服手动设计攻击的限制，实现可扩展的对抗输入生成。
### ​（3）越狱防御

针对VLM的越狱防御方法可分为**越狱检测（Jailbreak Detection）​**与**越狱预防（Jailbreak Prevention）​**两类。检测方法通过识别有害输入或输出来拦截或净化内容，预防方法则通过安全对齐或过滤机制增强模型对越狱查询的固有鲁棒性。

---

#### ​**a. 越狱检测**

1. ​**JailGuard [254]**
    
    - ​**方法**：通过对不可信输入进行**变异（mutation）​**并分析模型响应的不一致性来检测越狱攻击。
    - ​**特点**：使用18种针对文本和图像的变异器，提升对多种攻击类型的泛化检测能力。
2. ​**GuardMM [255]**
    
    - ​**方法**：两阶段防御框架
        - ​**阶段1**：输入验证，检测不安全内容；
        - ​**阶段2**：针对图像攻击的提示注入检测。
    - ​**特点**：采用专用语言强制执行安全规则。
3. ​**MLLM-Protector [257]**
    
    - ​**方法**：通过轻量级检测器识别有害响应，并通过专用转换机制进行去毒化（detoxify）。
    - ​**优势**：模块化设计，可轻松集成至现有VLM系统。

---

#### ​**b. 越狱预防**

1. ​**AdaShield [256]**
    
    - ​**方法**：在输入前**前置防御提示（defense prompts）​**，通过VLM与基于LLM的提示生成器协作自适应优化提示，无需微调。
    - ​**适用场景**：防御基于结构的越狱攻击（如恶意指令重组）。
2. ​**ECSO [258]**
    
    - ​**方法**：将不安全图像转换为文本描述，激活VLM内预训练LLM的安全对齐能力，确保输出安全性。
    - ​**特点**：无需训练的防护机制（training-free）。
3. ​**InferAligner [259]**
    
    - ​**方法**：在推理阶段应用**跨模型引导（cross-model guidance）​**，利用安全向量调整激活值生成安全可靠输出。
4. ​**BlueSuffix [260]**
    
    - ​**方法**：基于强化学习的黑盒防御框架，包含三个核心组件：
        - ​**图像净化器**：保护视觉输入安全；
        - ​**文本净化器**：保护文本输入安全；
        - ​**强化微调后缀生成器**：利用双模态梯度增强跨模态鲁棒性。

### （4）能耗-延迟攻击（Energy-Latency Attacks）​

与大型语言模型（LLMs）类似，多模态大语言模型（MMLMs）同样面临高计算需求的挑战。​**冗余图像（Verbose images）​**[261] 通过**过度占用服务资源**来利用这一特性，导致服务器成本上升、推断延迟增加以及GPU利用率降低。这类图像通过**延迟句子结束（EOS）标记的出现**，迫使自回归解码器的调用次数增加，从而显著提升能耗与延迟成本。
### （5） 提示注入攻击（Prompt Injection Attacks）​

针对视觉语言模型（VLMs）的提示注入攻击与针对LLM的攻击（见第3章）目标一致，但视觉模态的连续性特征使其更易通过对抗攻击或直接注入被利用。此类攻击可分为两类：​**基于优化的攻击（Optimization-based Attacks）​**与**基于排版的攻击（Typography-based Attacks）​**。

---

#### ​**a. 基于优化的攻击**

此类攻击通常利用（白盒）梯度优化输入图像以增强攻击效果，操控模型响应并影响后续交互。

- ​**Adversarial Prompt Injection**[262]：通过向图像添加对抗性扰动，将恶意指令嵌入VLM中。

---

#### ​**b. 基于排版的攻击**

利用VLM的排版漏洞，在无需梯度访问（黑盒）的情况下通过图像嵌入欺骗性文本实现攻击。

- ​**Typographic Attack**[263]：包含两种变体：
    1. ​**基于类别的攻击（Class-Based Attack）​**：误导模型错误分类；
    2. ​**描述性攻击（Descriptive Attack）​**：生成误导性标签。
- ​**风险扩展**：此类攻击还可导致**个人信息泄露**[475]，凸显重大安全风险。

### ​（6）后门攻击与投毒攻击

大多数视觉语言模型（VLMs）依赖视觉语言预训练（VLP）编码器，其安全威胁已在第4节讨论。本节重点关注**微调与测试阶段**​（尤其是视觉编码器与LLM对齐时）产生的后门与投毒风险：

- ​**后门攻击**：在视觉或文本输入中嵌入触发器（triggers），诱导特定输出；
- ​**投毒攻击**：注入恶意图文对（image-text pairs）以降低模型性能。  
    尽管现有研究主要聚焦后门攻击，本节仍分述两类攻击。

---

#### ​**a. 后门攻击**

针对VLMs的后门攻击可分为**调优时后门**与**测试时后门**：

1. ​**调优时后门（Tuning-time Backdoor）​**
    
    - 在VLM指令微调（instruction tuning）阶段注入后门：
        - ​**MABA**[264]：通过**属性解释（attributional interpretation）​**添加领域无关的触发器，增强图像描述任务中跨不匹配领域的攻击鲁棒性；
        - ​**BadVLMDriver**[266]：针对自动驾驶的物理后门攻击（如使用红色气球作为触发器），绕过数字防御触发危险行为（如突然加速）；
        - ​**ImgTrojan**[267]：毒化训练数据中的图文对，将描述替换为恶意提示以越狱VLM，揭示数据污染风险。
2. ​**测试时后门（Test-time Backdoor）​**
    
    - 利用通用对抗扰动（universal adversarial perturbations）与后门触发器的相似性，在测试阶段注入后门：
        - ​**AnyDoor**[265]：通过对抗测试图像在文本模态嵌入触发器，从图像-扰动组合创建文本后门，视为**多模态通用对抗攻击**。
        - ​**特点**：无需访问训练数据，攻击者可分阶段设置与激活攻击。

---

#### ​**b. 投毒攻击**

- ​**Shadowcast**[268]：针对VLM的隐蔽调优时后门攻击，注入视觉上与良性样本无异的毒化样本，目标包括：
    1. ​**标签攻击（Label Attack）​**：误导对象分类；
    2. ​**说服攻击（Persuasion Attack）​**：生成误导性叙述。
- ​**效果**：仅需50个毒化样本即可高效攻击，在黑盒场景下对多种VLM具有鲁棒性与迁移性。


###  VLM安全研究的数据集与基准

表2与表8详述了用于评估VLM安全性与鲁棒性的数据集与基准测试。以下是主要研究的总结：

1. ​**SafeSight[476]**
    
    - ​**数据集**：包含两个视觉问答（VQA）数据集——**OODCV-VQA**​（评估分布外鲁棒性）与**SketchyVQA**​（评估草图理解）。
    - ​**发现**：揭示VLM对**分布外（OOD）文本**的脆弱性及视觉编码器的弱点。
2. ​**MM-SafetyBench[477]**
    
    - ​**目标**：评估基于图像操作的多模态交互安全性。
    - ​**结论**：暴露VLM在多模态交互中的漏洞。
3. ​**AVIBench[478]**
    
    - ​**规模**：包含26万条对抗性视觉指令（AVIs），涵盖**基于图像**、**基于文本**及**内容偏见**的攻击类型。
    - ​**发现**：VLM对各类对抗性视觉指令高度敏感。
4. ​**GPT-4o越狱评估[479]**
    
    - ​**方法**：通过多模态与单模态越狱攻击测试GPT-4o的安全性。
    - ​**结论**：揭示GPT-4o在对齐机制上的漏洞。
5. ​**JailBreakV-28K[438]**
    
    - ​**目标**：验证LLM越狱技术在VLM中的迁移性。
    - ​**结果**：在10个开源VLM中均显示出高攻击成功率。

**综合结论**：上述研究共同表明，VLM在**分布外输入（OOD）​**、**对抗性指令**及**多模态越狱攻击**中存在显著安全漏洞。

## ​五.扩散模型

本节聚焦于扩散模型（Diffusion Models）[480]–[483]的安全性研究。扩散模型基于**正向噪声添加（forward noise addition）​**与**反向采样（reverse sampling）​**机制：

- ​**正向过程**：逐步向图像添加高斯噪声，直至其变为纯噪声；
- ​**反向采样**：基于学习到的数据分布，通过**逐步去噪（stepwise denoising）​**[484]–[486]生成新样本。

通过整合输入信息，扩散模型可实现**条件生成（conditional generation）​**，将数据分布建模从 p(x) 转化为 p(x∣guidance)。此类模型广泛应用于图像到图像（I2I）、文本到图像（T2I）、文本到视频（T2V）等任务，如内容创作、图像编辑与影视制作。然而，其广泛应用也带来了多种安全风险：

#### ​**攻击类型**

1. ​**对抗攻击（Adversarial Attacks）​**：破坏生成质量或绕过安全过滤器；
2. ​**越狱攻击（Jailbreak Attacks）​**：操控输出内容；
3. ​**后门攻击（Backdoor Attacks）​**：通过特定触发器操纵生成结果；
4. ​**隐私攻击（Privacy Attacks）​**：泄露敏感训练数据。

#### ​**防御措施**

1. ​**越狱与后门防御**：针对攻击机制设计检测与阻断方法；
2. ​**知识产权保护技术**：防止模型生成内容被滥用或盗用。

### **（1） 对抗攻击**

针对扩散模型的对抗攻击通常通过扰动文本提示词（text prompts）来**降低生成图像质量**或**导致语义与原始文本不匹配**。根据威胁模型，现有攻击方法可分为白盒、灰盒与黑盒三类。

---

#### ​**a.白盒攻击**

白盒攻击假设攻击者完全掌握目标T2I（文本到图像）扩散模型的参数，可直接优化文本提示或隐空间（latent space）以破坏图像生成。

- ​**SAGE[270]**：通过优化离散的文本提示与隐空间，揭示T2I模型的生成失败模式（如图像扭曲和目标篡改）。
- ​**ATM[269]**：利用**Gumbel Softmax**替换或扩展词汇，生成与干净提示相似的攻击性提示词，阻止模型生成预期对象。

---

#### ​**b. 灰盒攻击**

灰盒攻击假设T2I扩散模型使用的**CLIP文本编码器**被冻结且公开可用，攻击者可利用CLIP相似性损失（similarity loss）生成对抗性文本提示。

- ​**QFA[271]**：最小化原始文本与对抗文本嵌入的余弦相似度，使生成图像尽可能偏离原文本语义。
- ​**RVTA[272]**：最大化图像-文本相似度，使对抗提示与代理扩散模型生成的参考图像对齐。
- ​**MMP-Attack[487]**：在文本和图像模态中同时最大化对抗文本嵌入与目标嵌入的余弦相似度，采用**直通估计器（straight-through estimator）​**执行优化过程。

---

#### ​**c. 黑盒攻击**

黑盒攻击假设攻击者不了解目标扩散模型的内部参数或架构，仅通过文本对抗技术规避模型，分为三类：

1. ​**字符级攻击（Character-level Attacks）​**
    
    - ​**ECB[273]**：利用同形异义字符（如韩文或阿拉伯文字符）替换原字符，使生成图像偏向文化刻板印象。
    - ​**CharGrad[274]**：基于梯度优化字符级扰动，将字符变化映射到嵌入空间偏移。
    - ​**ER[275]**：使用基于分布的优化目标（如MMD、KL散度）最大化图像分布的差异性。
2. ​**词级攻击（Word-level Attacks）​**
    
    - ​**DHV[276]**：利用扩散模型中的“隐式词汇”（如用无意义字符串`Apoploe vesrreaitais`生成鸟类图像），揭示CLIP文本嵌入空间中词汇与概念的脆弱关联。
    - ​**AA[277]**：引入混合语言提示（macaronic prompting），结合多语言词片段系统操控生成结果。
3. ​**句级攻击（Sentence-level Attacks）​**
    
    - ​**RIATIG[279]**：以CLIP图像相似度为优化目标，结合遗传算法迭代调整文本提示，生成与目标图像相似但语义偏离的对抗样本。
    - ​**BBA[278]**：采用分类损失与黑盒优化改进提示词，通过**令牌空间投影（Token Space Projection, TSP）​**消除文本嵌入与离散令牌间的鸿沟，无需显式类别词即可生成特定类别图像。

### **（2） 越狱攻击（Jailbreak Attacks）​**

扩散模型通过**内部安全机制**​（训练阶段的安全对齐）和**外部安全机制**​（生成后的文本/图像过滤器）防止生成不适内容（NSFW）。越狱攻击旨在构造对抗性提示词绕过这些机制，生成有害内容。本节按威胁模型分类梳理现有方法：

---

#### ​**a. 白盒攻击**

假设攻击者完全访问模型参数，通过**梯度优化**绕过安全机制：

1. ​**内部安全攻击（Internal Safety Attacks）​**
    
    - ​**P4D**[282]：通过红队工具自动识别问题提示，将无约束模型的预测噪声与安全增强模型对齐。
    - ​**UnlearnDiffAtk**[283]：利用未学习扩散模型的分类能力优化对抗提示，强制模型在去噪过程中重建NSFW内容。
2. ​**外部安全攻击（External Safety Attacks）​**
    
    - ​**RTSDSF**[280]：利用CLIP模型编码NSFW词汇嵌入，通过字典攻击和“提示稀释”（添加无关细节）绕过过滤器。
    - ​**MMA**[281]：优化对抗提示并扰动输入图像，绕过图像编辑时的提示过滤器和后验安全检查器。

---

#### ​**b. 灰盒攻击**

假设攻击者仅能访问开源文本编码器，其他组件不可见：

1. ​**内部安全攻击**
    - ​**Ring-A-Bell**[488]：通过反义词提示对提取不安全概念，结合遗传算法生成有害提示。
    - ​**JPA**[284]：计算“裸露”与“穿衣”在文本嵌入空间的差异，优化前缀提示实现语义对齐。
    - ​**RT-Attack**[285]：两阶段策略最大化与NSFW提示的文本相似性，迭代优化攻击提示。

---

#### ​**c.黑盒攻击**

仅能访问模型输出（如过滤拒绝信号或生成图像），针对商业模型：

1. ​**外部安全攻击（External Safety Attacks）​**
    - ​**手工设计**：
        - ​**UD**[287]：手动优化NSFW提示生成仇恨类内容。
        - ​**SneakyPrompt**[286]：强化学习优化对抗提示，基于过滤器规避和语义对齐更新策略网络。
    - ​**LLM辅助**：
        - ​**Groot**[289]：分解提示为对象与属性，稀释敏感内容。
        - ​**DACA**[290]：利用LLM解构并重组提示。
        - ​**Atlas**[288]：双智能体系统（VLM生成对抗提示 + LLM评估筛选）。

### **（3） 越狱防御**

本节梳理针对T2I扩散模型的越狱攻击防御策略，包括**概念擦除（Concept Erasure）​**与**推理指导（Inference Guidance）​**。核心挑战在于**平衡安全性与生成质量**。

---
#### ​**a. 概念擦除**

旨在从扩散模型中移除不良概念（如NSFW内容、版权风格），分为三类：

##### ​**基于微调的方法**

通过梯度优化调整模型参数，结合**擦除损失项**​（抑制目标概念）与**约束项**​（保留非目标概念）：

1. ​**基于锚点的擦除（Anchor-based Erasing）​**
    
    - ​**AC**[295]：将目标概念（如“不爽猫”）对齐到更广的锚点概念（如“猫”），利用锚点的文本-图像对保留正常生成能力。
    - ​**ABO**[296]：通过修改分类器引导，显式（替换目标）或隐式（抑制注意力图）擦除信号，并添加惩罚项保护生成质量。
    - ​**DoCo**[297]：对抗训练对齐目标与锚点概念，通过梯度手术缓解概念冲突。
    - ​**SPM**[293]：1D适配器+负向引导（negative guidance）[292]，仅抑制目标概念的同义词。
    - ​**SA**[298]：生成式回放与弹性权重整合，稳定模型权重并保留非目标概念。
2. ​**无锚点擦除（Anchor-free Erasing）​**
    
    - ​**ESD**[292]：将分类器无关引导（classifier-free guidance）改为负向噪声预测，最小化目标概念（如“梵高”）生成概率。
    - ​**SDD**[294]：利用无条件预测与EMA缓解负向引导的副作用。
    - ​**DT**[302]：通过去噪低频扰动图像擦除不安全概念。
    - ​**Forget-Me-Not**[303]：注意力重定向（Attention Resteering），最小化与目标概念相关的中间注意力图。
    - ​**Geom-Erasing**[304]：几何驱动控制方法擦除隐式概念（如水印）。
3. ​**对抗性擦除（Adversarial Erasing）​**
    
    - ​**Receler**[299]：轻量擦除器+对抗提示嵌入，通过U-Net注意力图掩码定位概念区域。
    - ​**AdvUnlearn**[301]：对文本编码器进行对抗攻击，正则化保护正常生成。
    - ​**RACE**[300]：单时间步对抗攻击，降低计算开销。

##### ​**闭式解方法**

基于交叉注意力层的局部更新实现高效擦除（无需微调）：

- ​**TIME**[308]：闭式解去偏模型。
- ​**UCE**[307]：扩展至多目标擦除，保留周边概念以减少干扰。
- ​**MACE**[306]：结合LoRA与Grounded-SAM[408][491]实现区域特定擦除。
- ​**RealEra**[310]：挖掘关联概念，添加嵌入扰动+正则化扩展擦除范围。
- ​**RECE**[309]：持续发现新概念嵌入并进一步擦除。

##### ​** 基于剪枝的方法**

识别并移除与目标概念强相关的神经元：

- ​**ConceptPrune**：计算Wanda分数评估神经元贡献，剪枝目标相关神经元。
- ​**其他方法**[312]：对抗提示增强擦除鲁棒性。

---

#### ​**b. 推理指导**

在推理过程中引入额外信息引导生成安全内容：

1. ​**输入指导（Input Guidance）​**
    
    - ​**SLD**[314]：基于文本条件与不安全概念调整噪声预测，避免微调。提出I2P基准测试数据集。
2. ​**输入输出联合指导（Input & Output Guidance）​**
    
    - ​**Ethical-Lens**[315]：即插即用框架，LLM修订输入文本+多头CLIP分类器修改输出图像，确保符合社会价值观。
3. ​**潜在空间指导（Latent Space Guidance）​**
    
    - ​**SDIDLD**[316]：自监督学习识别不当概念的相反潜在方向（如“反色情”），在瓶颈层添加向量阻止有害内容生成。

### **（4） 后门攻击**

扩散模型的后门攻击通过在训练阶段注入**后门触发器（backdoor triggers）​**，使攻击者能在生成阶段通过触发输入（如特定提示词或初始噪声）操控生成内容。核心挑战在于**提高攻击成功率的同时保持触发器的隐蔽性与模型原有功能**。现有攻击方法分为**训练操纵**与**数据投毒**两类。

---

#### ​**a.训练操纵（Training Manipulation）​**

此类攻击假设攻击者能控制扩散模型的训练或推理过程，主要针对视觉模态，通过带触发器的图像对与目标图像（image-image pair injection）注入后门，常用于**无条件扩散模型**。

- ​**BadDiffusion**[317]：首个针对T2I扩散模型的后门攻击，通过修改前向加噪与反向去噪过程，将后门目标分布映射到图像触发器，同时保持DDPM采样机制。
- ​**VillanDiffusion**[318]：扩展至条件模型，添加基于提示的触发器与文本触发器，适用于文本到图像生成任务。
- ​**TrojDiff**[319]：通过控制训练与推理过程，在采样中引入特洛伊噪声，实现多样化攻击目标。
- ​**IBA**[320]：通过双层优化生成不可见触发器，规避检测。
- ​**DIFF2**[321]：在对抗净化（adversarial purification）中优化触发器误导分类器，并通过数据投毒直接注入后门。

---

#### ​**b.数据投毒（Data Poisoning）​**

数据投毒不直接干预训练过程，仅通过向数据集中注入毒化样本实现攻击，主要针对**条件扩散模型**，探索两类文本触发器：​**文本-文本对**与**文本-图像对**。

1. ​**文本-文本对触发器（Text-text Pair Triggers）​**
    
    - ​**RA**[322]：在文本编码器中添加隐蔽触发字符，通过优化效用损失（utility loss）将原始提示映射为目标提示，引导扩散模型生成特定语义内容。
    - ​**其他研究**[322][325]-[327]：类似方法验证文本编码器的后门注入可行性。
2. ​**文本-图像对触发器（Text-image Pair Triggers）​**
    
    - ​**BadT2I**[323]：基于像素、对象或风格变化的触发器（如`[T]`），诱导生成含特定补丁、替换对象或风格的图像。
    - ​**ZeroDay**[326][327]：通过个性化微调注入触发图像对，降低数据成本。
    - ​**FTHCW**[324]：将目标模式嵌入不同类别图像，形成文本-图像对生成多样化输出。
    - ​**IBT**[329]：使用双词触发器（需同时出现）激活后门，增强隐蔽性。
    - ​**BAGM**[325]：商业场景中，将宽泛术语（如“饮料”）映射至特定品牌（如“可口可乐”），操控用户情感。
    - ​**SBD**[328]：通过分解与重组受版权保护内容，利用文本-图像对绕过过滤器，实施版权侵权。

### **（5） 后门防御**

扩散模型的后门防御是新兴研究领域，当前方法通常遵循三步流程：​**1) 触发器反演（Trigger Inversion）​**、**2) 触发器验证或后门检测**、**3) 后门移除**。部分研究提出完整框架，其他则聚焦于特定步骤。

---

#### ​**a. 后门检测**

早期研究主要集中于检测或验证后门触发器：

- ​**T2IShield**[330]：首个针对扩散模型的后门检测与缓解框架，利用**交叉注意力图（cross-attention maps）​**中的**同化现象（assimilation phenomenon）​**​（触发词抑制其他标记生成特定内容）。
- ​**Ufid**[331]：通过对比干净生成（对微小扰动敏感）与后门触发输出（鲁棒性更高）验证触发器。
- ​**DisDet**[332]：低成本检测方法，通过识别分布偏移区分中毒输入噪声与干净高斯噪声。

---

#### ​**b.后门移除**

验证后门存在后需从模型中移除触发器，多数方法通过反演触发器并消除其影响：

- ​**Elijah**[333]：通过分布偏移反演触发器，将后门分布对齐至干净分布。
- ​**DiffCleanse**[334]：将触发器反演建模为结合相似性与熵损失的优化问题，并剪枝与后门采样相关的关键通道。
- ​**TERD**[335]：提出统一反向损失（reverse loss）进行触发器反演，分粗粒度与细粒度两阶段优化。
- ​**PureDiffusion**[336]：利用多时间步触发器反演，基于后门前向过程引发的分布一致性偏移。

---

#### ​**隐私攻击分类**

扩散模型的隐私攻击可分为三类：

1. ​**成员推断攻击（Membership Inference）​**：推断特定数据是否用于训练模型。
2. ​**数据提取攻击（Data Extraction）​**：从模型输出中恢复训练数据。
3. ​**模型提取攻击（Model Extraction）​**：复制或逆向工程模型参数。  
    随着攻击技术演进，每类攻击对隐私的威胁日益严峻。

### **(6)  成员推断攻击（Membership Inference Attacks）​**

成员推断攻击旨在利用扩散模型的生成能力推断敏感数据（如判断某样本是否属于训练集）。攻击方法基于**重构误差、影子模型、辅助数据、损失值、梯度或结构相似性**等指标，可分为六类：

---

#### ​**a.基于重构误差的攻击（Reconstruction Error-based Attacks）​**

通过分析样本在扩散模型中的重构误差推断其成员身份：

- ​**Wu等人[347]**：针对文本条件扩散模型，通过比较候选样本与生成图像的重构误差及其与文本提示的语义对齐性判断成员身份。
- ​**Matsumoto等人[339]**：提出**Diffusionleaks**，生成多个候选图像，基于最小重构误差推断成员身份（灵感来自GAN-Leaks[492]）。
- ​**Li等人[349]**：通过平均多次重构结果降低误差，利用黑盒API修改候选图像提升推断精度。
- ​**DRC[350]**：通过扩散模型退化并恢复图像，比较恢复图像与原始图像的差异推断成员身份及敏感特征。

---

#### ​**b. 基于辅助数据集的攻击（Auxiliary Datasets-based Attacks）​**

利用辅助数据集训练影子模型（shadow models），通过模拟目标模型实现黑盒成员推断：

- ​**Pang等人[348]**：针对微调的条件扩散模型，计算查询图像与生成图像的相似性得分，训练二元分类器进行成员推断。
- ​**GMIA[351]**：首个生成模型的通用成员推断攻击，仅需生成分布与辅助非成员数据集，假设生成分布近似原始训练分布。

---

#### ​**c.基于损失值的攻击（Loss-based Attacks）​**

利用损失值分布区分成员（训练）与非成员样本（假设成员样本损失更低）：

- ​**[340]与[339]**：利用不同时间步的损失值进行推断（视为**静态损失攻击SLA**，忽略扩散过程）。
- ​**Dubinski等人[337]**：修改扩散过程从多视角提取损失信息，提升推断精度。

---

#### ​**d.基于梯度的攻击（Gradient-based Attacks）​**

利用梯度信息推断成员身份：

- ​**GSA[338]**：若样本梯度与周围样本差异显著（表明对模型训练影响更大），则判定为成员。

---

#### **​f.基于结构相似性的攻击（Structural Similarity-based Attacks）​**

比较候选样本与模型输出的结构特征或相似性指标：

- ​**SMIA[346]**：利用结构相似性指标（SSIM[493]）评估图像在扩散过程中的结构保留程度，通过成员与非成员的平均SSIM差异推断成员身份。

---

#### ​**g.基于似然的攻击（Likelihood-based Attacks）​**

利用后验或条件似然推断成员身份：

- ​**SecMI[341]**：通过逆过程估计后验似然，针对DDPM和Stable Diffusion模型。
- ​**QRMI[342]**：对后验似然应用分位数回归。
- ​**SIA[494]**：基于逆扩散过程中噪声参数的差异推断成员身份。
- ​**PIA[343]**：利用扩散模型特性以更少查询完成推断。
- ​**PFAMI[344]**：分析目标样本与邻域的波动，利用生成模型的记忆特性。
- ​**Zhai等人[345]**：利用条件似然差异（因过拟合导致）推断成员身份。

### **(7) 数据提取攻击（Data Extraction Attacks）​**

数据提取攻击旨在通过逆向工程从训练好的扩散模型中提取原始训练数据或属性，其有效性依赖于模型对特定属性的记忆能力[495]–[498]。根据条件类型，此类攻击可分为两类：​**基于显式条件的数据提取**与**基于替代条件的数据提取**。

---

#### ​**a.基于显式条件的数据提取（Explicit Condition-based Extraction）​**

利用T2I（文本到图像）扩散模型中的条件信息提取记忆的训练样本。攻击者通过特定文本提示生成与训练数据相似的图像：

- ​**[352]**：提出**暴力数据提取（BruteDE）​**，通过定向提示生成图像，并利用成员推断（membership inference）匹配训练数据，但效率较低。
- ​**一步提取（OSE）​**[353]：利用“模板复现”（template verbatims）现象，通过**去噪置信度得分（DCS）​**和**边缘一致性得分（ECS）​**快速筛选生成样本，加速数据提取。

---

#### ​**b. 基于替代条件的数据提取（Surrogate Condition-based Extraction）​**

通过构造替代条件从**无条件扩散模型**中提取数据：

- ​**SIDE**[354]：利用分类器或特征提取器的**隐式标签（implicit labels）​**作为替代条件，引导生成与训练数据相关的图像。
- ​**FineXtract**[355]：使用微调模型作为替代条件，在潜在空间中与微调数据相关的区域引导提取，实现定向数据恢复。


### **(8)模型提取攻击（Model Extraction Attacks）​**

模型提取攻击旨在窃取已训练扩散模型的内部参数或架构。目前已知的唯一扩散模型提取方法是**谱去调优（Spectral DeTuning, SDeT）​**[356]。

**SDeT方法原理**：

1. ​**低秩适配利用**：通过**低秩适配（Low-Rank Adaptation, LoRA）​**[499] 提取经LoRA微调的生成模型的**预微调权重**。
2. ​**多模型优化**：收集来自同一预训练模型的多个微调模型，构建优化问题，目标是在低秩约束下最小化微调权重与原始权重及适配矩阵之和的差异。
3. ​**求解方法**：采用**奇异值分解（Singular Value Decomposition, SVD）​**[500] 进行迭代求解。

**攻击效果**：

- SDeT可高效恢复如Stable Diffusion、Mistral-7B[501]等模型的原始权重，揭示了基于低秩适配的微调流程的安全漏洞。

### ​（9）知识产权保护

人工智能的知识产权保护是新兴研究领域，通过对抗攻击、水印技术等手段保护自然数据（训练/测试数据）、生成数据及训练模型的知识产权。现有方法通常假设对保护对象有完全访问权限，分为以下三类：

---

#### ​**a.自然数据保护**

保护自然采集数据的版权（非生成数据），防止模型所有者非法使用，分为三类：

1. ​**学习预防（Learning Prevention）​**
    
    - ​**DUAW[357]**：通过优化通用对抗扰动破坏Stable Diffusion的变分自编码器（VAE），扭曲输出以保护版权图像。
    - ​**AdvDM[358]**：生成对抗样本防止扩散模型模仿艺术风格。
    - ​**Anti-DreamBooth[359]**：向用户图像注入对抗噪声，阻止模型学习个性化特征。
    - ​**MetaCloak[360]**：利用代理扩散模型生成可迁移扰动，并通过去噪误差最大化提升鲁棒性。
2. ​**编辑预防（Editing Prevention）​**
    
    - ​**EditGuard[363]**：嵌入唯一水印，抵抗基于扩散模型的编辑（如背景移除、换脸）。
    - ​**WaDiff[364]**：为每个用户查询添加唯一水印，实现生成图像溯源。
    - ​**AdvWatermark[365]**：添加对抗噪声，在图像被篡改时显示可见签名。
3. ​**数据归属（Data Attribution）​**
    
    - ​**FT-SHIELD[366]**：通过交替优化和PGD[409]嵌入水印，结合二元检测器验证。
    - ​**DiffusionShield[367]**：将版权信息编码为水印块，联合优化解码器与水印块。
    - ​**ProMark[368]**：在训练数据中嵌入水印，当模型生成类似概念时提取验证。

---

#### ​**b.生成数据保护**

保护AI生成内容（AIGC）的版权，通过嵌入可验证水印标识创作者：

- ​**Stable Signature[371]**：微调解码器嵌入二进制签名，通过预训练提取器和统计检验验证。
- ​**LaWa[372]**：在潜在扩散模型中分层嵌入水印，结合对抗训练提升鲁棒性。
- ​**Safe-SD[373]**：在图像不可见结构中嵌入图形水印（如二维码），提升可追溯性。

**攻击与脆弱性**：

- ​**WEvade[502]**：通过添加细微扰动绕过水印检测。
- ​**TAIW[503]**：在无盒（no-box）设置下利用代理水印模型进行迁移攻击。
- ​**SSU[504]**：通过微调解码器移除生成过程中的水印，暴露水印技术脆弱性。

---

#### ​**c.模型保护**

保护已发布模型的知识产权，支持所有权验证与内容溯源：

1. ​**模型水印（Model Watermark）​**
    
    - ​**Zhao等人[374]**：针对无条件/类条件模型，预训练水印编码器嵌入二进制字符串；针对T2I模型，使用（文本，图像）触发器（如QR码）。
    - ​**FIXEDWM[375]**：固定触发器在提示中的位置以增强隐蔽性。
    - ​**WDM[376]**：通过水印扩散过程（WDP）嵌入水印，训练时混合标准与水印扩散路径。
2. ​**模型归属（Model Attribution）​**
    
    - ​**Tree-Ring[379]**：在T2I生成的初始高斯噪声的傅里叶空间嵌入水印，通过DDIM反演提取验证。
    - ​**AquaLoRA[377]**：向模型参数嵌入密钥字符串，防止恶意用户篡改水印。
    - ​**LatentTracer[378]**：通过逆向工程潜在输入溯源生成模型，无需人工指纹或水印。


### ​(10) 数据集

本节总结扩散模型安全研究中的常用数据集（详见表9和表10）：

#### ​**1. 对抗攻击与防御研究**

- ​**条件扩散模型**：主要使用带标注的文本-图像对数据集，如：
    - ​**MS COCO**[418]
    - ​**LAION**[505][506]
    - ​**DiffusionDB**[507]
- ​**无条件扩散模型**：常用分类任务数据集评估攻击效果与生成质量，如：
    - ​**ImageNet**[508]
    - ​**CIFAR10/100**[509]

#### ​**2. NSFW内容研究**

- ​**I2P数据集**[314]：广泛用于测试不适当内容生成。
- ​**自定义数据集**：
    - ​**NSFW200**[286]：包含200个NSFW样本。
    - ​**VBCDE-100**[290]：针对视觉偏见与伦理冲突的100个样本。
    - ​**Tox100/1K**[315]：包含100或1000个有毒内容样本。
    - ​**人类属性数据集**[315]：用于研究模型生成内容中的社会偏见。

#### ​**3. 知识产权保护**

- ​**人脸数据集**：
    - ​**CelebA**[510]
    - ​**VGGFace2**[511]
- ​**对象数据集**：
    - ​**DreamBooth**[512]：用于个性化生成任务的定制数据集。
    - ​**Pokemon Captions**[513]：宝可梦相关图文对数据集。
- ​**艺术风格数据集**：
    - ​**WikiArt**[514]：涵盖多种艺术流派的图像数据集。

## 六.智能体安全（Agent Safety）​

基于大模型的智能体（如大语言模型LLM、视觉语言模型VLM驱动的智能体）正被广泛应用于各类场景，尤其在**安全关键领域**​（如医疗机器人、自动驾驶）中解决复杂问题。确保其安全性至关重要。本节系统梳理现有智能体安全研究，总结核心挑战并强调持续创新的必要性。现有研究可大致分为两类：​**大语言模型（LLM）智能体安全**与**视觉语言模型（VLM）智能体安全**。

### **1.大语言模型（LLM）智能体安全**

本节从**攻击方法**、**防御机制**和**基准测试**三方面梳理LLM智能体安全研究。

---

#### ​（1） 攻击方法

理解并缓解LLM智能体的脆弱性对其安全部署至关重要，现有攻击主要分为三类：

1. ​**提示注入攻击（Prompt Injection Attacks）​**
    
    - ​**原理**：通过向输入提示中嵌入恶意指令操控智能体行为。
    - ​**间接提示注入（IPI）​**：将恶意指令隐藏在外部内容（如网页、邮件、文档）中，利用智能体对外部信息的依赖触发非预期行为。
        - ​**InjecAgent[380]**：通过产品评论中的隐藏指令操控智能体执行危险操作。
        - ​**Breaking Agents[381]**：揭示智能体多组件（如记忆、工具调用）的广泛脆弱性。
2. ​**后门攻击（Backdoor Attacks）​**
    
    - ​**原理**：在模型或知识库中植入隐藏触发器，特定条件下触发恶意行为。
        - ​**BadAgent[382]**：通过毒化微调数据注入后门。
        - ​**Contextual Backdoor Attacks[384]**：利用看似良性的上下文示例在特定场景触发攻击。
        - ​**AgentPoison[383]**：针对智能体记忆或知识库，通过预定义触发器激活恶意操作。
3. ​**越狱攻击（Jailbreak Attacks）​**
    
    - ​**原理**：绕过安全机制与伦理准则，诱导智能体执行受限操作。
        - ​**PsySafe[385]**：利用对抗提示激发多智能体系统中的“黑暗人格特质”，覆盖安全协议。

---

#### ​（2） 防御机制

现有防御方法分为两类：

1. ​**响应过滤（Response Filtering）​**
    
    - ​**AutoDefense[387]**：多代理防御框架，通过角色分工协作分析智能体响应，基于共识决策过滤有害内容。
    - ​**TrustAgent[386]**：三阶段安全策略：
        1. ​**预规划（Pre-planning）​**：注入安全知识；
        2. ​**规划中（In-planning）​**：生成时增强安全性；
        3. ​**后规划（Post-planning）​**：执行前检查输出合规性。
2. ​**知识增强推理（Knowledge-Enabled Reasoning）​**
    
    - ​**GuardAgent[388]**：引入守护代理（Guard Agent），基于知识库与工具集将动作计划转换为可执行代码，验证合规性。

---

#### ​（3）基准测试

现有基准测试评估智能体在交互环境中的风险识别与防御能力：

1. ​**RJudge[389]**：
    
    - ​**规模**：569条记录，覆盖27个场景与10类风险。
    - ​**目标**：测试智能体从交互日志中识别安全风险的能力。
2. ​**SafeAgentBench[391]**：
    
    - ​**任务**：750个任务，模拟环境中不同危害等级与抽象层级的场景。
    - ​**指标**：任务执行成功率与语义理解准确性。
3. ​**AgentDojo[390]**：
    
    - ​**攻击测试**：97个任务与629个安全测试案例，评估智能体对第三方数据中恶意指令的鲁棒性。

### **2. 视觉语言模型（VLM）智能体安全**

VLM智能体结合视觉感知与语言理解能力，面临独特的安全挑战。当前针对VLM智能体的攻击主要从**白盒攻击**、**黑盒攻击**和**鲁棒性分析**三方面展开。尽管攻击手段多样，但有效防御策略仍处于探索阶段。

---

#### ​(1) 攻击方法

1. ​**白盒攻击（White-box Attacks）​**
    
    - ​**原理**：攻击者完全掌握模型参数，通过梯度优化暴露理论漏洞。
        - ​**Fu等人[392]**：利用梯度训练与特征刻画生成对抗图像，诱导LLM执行真实工具命令，破坏用户隐私与数据完整性。
        - ​**Tan等人[393]**：在多智能体社会中，通过白盒生成的恶意提示词实现智能体间越狱攻击，扩散有害输出。
2. ​**黑盒攻击（Black-box Attacks）​**
    
    - ​**原理**：仅依赖输入-输出行为生成对抗样本，更贴近实际场景。
        - ​**AgentSmith[394]**：利用单张对抗图像越狱多个VLM智能体，借助其互联性在网络中指数级传播恶意行为。

---

#### ​(2) 鲁棒性分析（Robustness Analysis）​

- ​**ARE[395]**：提出**智能体鲁棒性评估框架**，将VLM智能体建模为图结构，分析组件间的**对抗性影响流（adversarial influence flow）​**，识别系统弱点并评估组件修改对安全性的影响。

## 七.挑战
通过系统调研，我们总结了现有研究的**局限性与研究空白**，并归纳为以下关键议题。这些**开放挑战**反映了大模型安全领域的**动态演变本质**，揭示了在确保各类AI系统**鲁棒性与可靠性**的进程中，亟待突破的**技术壁垒与方法论瓶颈**。

#### **7.1 基础性漏洞**

探索和理解大模型的基础性漏洞对于开发鲁棒的防御和安全框架至关重要。本节重点分析各类大模型的核心弱点和固有挑战。

---

##### ​**7.1.1 攻击的目标不仅是破坏模型**

现有研究大多聚焦于设计攻击以破坏模型功能，但攻击研究的真正目标应超越单纯的破坏。攻击可作为**诊断工具**，揭示模型的意外行为及其决策过程的根本性缺陷。通过理解模型的失败机制，我们才能从根源解决漏洞，而非表面修复。针对每个新攻击，需回答以下关键问题：

- ​**攻击为何成功或失败？**
- ​**如何利用模型特性？**
- ​**揭示了哪些未知漏洞？**
- ​**这些漏洞是模型架构的固有缺陷还是特定类别的共性问题？**  
    这些问题通过暴露系统性缺陷（而非孤立问题）指导更鲁棒的模型与防御开发。

---

##### ​**7.1.2 语言模型的基础性漏洞是什么？**

ChatGPT、Gemini等LLMs的漏洞源于其对**统计模式而非语义理解**的依赖[515]，主要弱点包括：

1. ​**对抗输入的敏感性**
2. ​**训练数据的偏见性**
3. ​**通过提示工程（Prompt Engineering）的操控性**  
    需深入探究这些漏洞如何从模型架构和训练数据中产生：

- ​**训练数据记忆化**：导致隐私泄露或数据泄漏。
- ​**有害内容暴露**：传播偏见或毒性输出。
- ​**幻觉放大**：生成看似合理但错误或无意义的信息。

**开放问题**：

- 文本输入的离散性是否使语言模型比视觉模型更鲁棒？
- 越狱攻击和数据提取攻击揭示了哪些基础性漏洞？

---

##### ​**7.1.3 漏洞如何跨模态传播？**

多模态大语言模型（MLLMs）整合不同模态时，新漏洞随之产生：

- ​**视觉编码器**对像素空间的细微扰动敏感。
- ​**语言模型**易受对抗字符、词汇或提示词攻击。  
    **核心挑战**：

1. 某模态（如视觉）的漏洞如何影响另一模态（如语言）的行为？
2. 跨模态的标记数量如何影响漏洞传播？
3. 如何通过统一框架解决跨模态漏洞（而非单模态防御）？

---

##### ​**7.1.4 视觉生成扩散模型缺乏语言能力**

图像/视频生成的扩散模型擅长视觉创作，但缺乏语言理解能力（与VLP模型类似）。其核心架构仅优化像素级生成任务，导致可能生成**有害或上下文不匹配的内容**。未来需整合语言理解能力，但可能引入新漏洞（如通过细粒度操控生成过程的攻击）。

---

##### ​**7.1.5 模型的训练数据记忆化程度**

深度神经网络（DNNs）的记忆化能力引发隐私攻击（如成员推断、数据提取）的担忧。LLMs和DMs均被证明可在特定条件下复制并泄露部分训练数据。  
**开放问题**：

- DNNs是否主要通过记忆化学习？
- 记忆化的“开关”机制是什么？
- 如何有效测量记忆化（精确匹配、训练等价性还是嵌入相似性）？

---

##### ​**7.1.6 智能体漏洞随能力增强而增长**

大模型驱动的智能体能力越强，漏洞风险越高[394]。其与外部工具、数据源和环境的交互扩大了攻击面，导致防御机制复杂化：

- ​**漏洞叠加效应**：基础模型漏洞在智能体决策中可能引发级联故障。
- ​**动态学习风险**：自适应能力可能暴露于隐性偏见或对抗输入，导致不可预测行为。

**研究方向**：

1. 理解模型组件（语言、视觉、决策）间的漏洞传播机制。
2. 开发动态演化环境中的智能体评估方法，抵御新兴威胁。


#### **7.2 安全评估**

全面且标准化的安全评估对量化大模型的安全性至关重要。然而，现有评估数据集和基准测试通常是静态的或仅聚焦于特定威胁。为确保模型在真实场景中可靠运行，安全评估需覆盖多样化且不可预测的情境。

---

##### ​**7.2.1 攻击成功率不足以衡量安全性**

攻击成功率（Attack Success Rate, ASR）虽常用于安全研究，但仅量化攻击破坏模型输出的频率，忽略了以下关键因素：

- ​**破坏的严重性**：模型输出可能轻微偏离或完全崩溃，但危害程度未被区分。
- ​**模型韧性（Resilience）​**：模型对不同类型攻击（如对抗样本、提示注入）的恢复能力。
- ​**实际后果**：攻击可能未导致明显故障，但会隐性扭曲决策（如医疗诊断错误）。

**改进方向**：需定义多层级、细粒度的漏洞指标，涵盖模型对攻击的敏感性、恢复能力及失效模式的伦理影响。

---

##### ​**7.2.2 静态评估会营造虚假的安全感**

现有安全评估依赖静态基准或开源数据集，但这些数据已被模型训练者和攻击者熟知，导致模型在过时数据上表现优异，但实际场景中鲁棒性不足。  
**局限性**：

- 无法捕捉动态演变的威胁（如新型对抗攻击）。
- 无法反映模型在未知场景中的真实风险。

**改进策略**：

1. ​**动态评估系统**：如**Chatbot Arena**[516]，随新威胁出现而持续更新测试案例。
2. ​**​“种子”生成法**：仅发布数据集的结构组件与生成方法，动态生成新测试案例。

---

##### ​**7.2.3 对抗评估是必要手段，而非可选**

常规安全测试虽能评估模型整体性能，但无法覆盖真实对抗场景中的风险。对抗评估通过模拟攻击条件揭示模型弱点：

- ​**对抗博弈框架**：基于强化学习的对抗智能体与目标模型交互，动态挖掘新漏洞。
- ​**商业API的特殊性**：过滤机制可能屏蔽恶意输入，需设计绕过过滤的评估方法以反映真实威胁。

---

##### ​**7.2.4 开放式评估的挑战**

大模型的开放式生成特性（如文本、图像）使攻击评估复杂化。例如，越狱攻击（Jailbreaking）的成功率难以量化，因输出空间无限且无明确类别标签。  
**现有评估方法的不足**：

- ​**基于规则**​（如关键词检测）：易被绕过（如替换同义词）。
- ​**基于模型**​（如GPT或Llama-Guard）：依赖检测模型自身的局限性。

**潜在解决方案**：

- 限制输出空间为有限动作集（类似智能体场景），简化评估复杂度。

---

##### ​**总结**

当前安全评估需从静态、单维度转向动态、多层级框架，结合对抗博弈与开放式测试，以全面衡量模型在真实场景中的鲁棒性。


#### **7.3 安全防御**

大模型的安全机制对防止有害或意外行为至关重要，这些机制可能涉及模型架构的修改或外部监控系统的整合。本节探讨开发鲁棒防御方案中的开放挑战。

---

##### ​**7.3.1 安全对齐并非万能解药**

安全对齐（确保模型目标与人类价值观一致）虽能缓解部分风险，但存在显著缺陷：

- ​**假对齐（Fake Alignment）​**[432][517]：模型可能通过表面优化获得高安全评分，但未真正理解安全原则。
- ​**浅层安全（Shallow Safety）​**：即使对齐良好的模型（如GPT-4o[518]）仍可被复杂攻击绕过[479]。

**开放挑战**：

1. 揭示安全对齐的机制性局限，开发确保深度安全的方法。
2. 超越表面指标（如首几个输出标记的分布），实现全面对齐。
3. 通过对抗性对齐（主动挑战模型安全机制）提升鲁棒性[519]。

---

##### ​**7.3.2 越狱攻击比对抗攻击更难防御**

越狱攻击与对抗攻击的区别：

- ​**目标**：越狱攻击旨在诱导有害输出，需绕过安全机制；对抗攻击不必然生成恶意内容。
- ​**扰动限制**：对抗攻击需保持扰动隐蔽（人类不可察觉），而越狱攻击无此限制，攻击设计更灵活。

**挑战**：需开发统一防御策略，应对两者的攻击灵活性差异。

---

##### ​**7.3.3 需要更实用的防御方案**

现有防御方法的局限性：

1. ​**缺乏通用性**：需跨模态（视觉、语言、多模态）的统一防御方案。
2. ​**黑盒兼容性**：防御需基于输入-输出行为，无需模型内部参数。
3. ​**效率不足**：对抗训练等方案计算成本过高，需平衡鲁棒性与资源消耗。
4. ​**持续适应性**：模型需实时更新防御策略，无需依赖大规模重训练。

**目标**：整合上述特性，构建兼顾性能与安全的防御框架。

---

##### ​**7.3.4 主动防御的缺失**

现有防御多为被动（如安全对齐、对抗训练），主动防御研究不足。​**潜在方向**：

- ​**对抗模型提取**：向提取尝试投毒或植入后门，使被盗模型不可靠。
- ​**误导性回应**：对恶意请求（如犯罪计划）提供无意义或易检测的响应，威慑攻击者。

**挑战**：设计针对不同威胁的主动防御策略。

---

##### ​**7.3.5 检测机制在防御中被忽视**

检测系统作为主动监控工具，可识别漏洞与异常行为，触发安全响应。​**关键问题**：

- 如何将检测深度整合为防御核心？
- 检测与其他机制（如过滤、重训练）如何互补？

**示例**：通过检测强攻击信号实现主动防御。

---

##### ​**7.3.6 数据使用实践亟需变革**

当前AI数据生态的三大问题：

1. ​**缺乏授权与认可**：网络爬取数据未经所有者同意。
2. ​**生成数据爆炸**：互联网充斥虚假、有害数据，且无法溯源。
3. ​**​“免费”数据枯竭**：用户因无回报拒绝贡献高质量数据。

**解决方向**：

- ​**数据归属技术**：
    - ​**成员推断（Membership Inference）​**：验证原始数据是否用于训练。
    - ​**模型溯源（Model Attribution）​**：识别生成数据的来源模型或用户。
    - ​**数据贡献追踪**：确定训练数据对生成内容的贡献度，建立公平收益分配机制。

---

##### ​**7.3.7 实体代理安全**

实体代理（如机器人、自动驾驶）在物理世界的部署带来新风险：

- ​**安全需求**：抗干扰输入、自我调节有害行为、动态对齐人类价值观。
- ​**挑战**：设计安全协议，使代理在复杂任务中自主运行且可信。

**伦理责任**：随着代理自主性增强，安全不仅是技术问题，更是伦理义务。

---

##### ​**7.3.8 超级智能安全**

向AGI与超级智能演进中，需内嵌安全机制以确保可控性。​**潜在方案**：

1. ​**监督系统（Oversight System）​**：
    - ​**监督悖论**：若监督系统需比超级智能更强大，则其自身可靠性如何保证？
2. ​**安全层（Safety Layer）​**[520][521]：
    - 在模型架构中嵌入动态更新的安全过滤器。
3. ​**安全专家（Safety Expert）​**[522]-[525]：
    - 在混合专家（MoE）框架中，优先路由高风险查询至安全专家。
4. ​**对抗对齐（Adversarial Alignment）​**：
    - 通过对抗训练暴露漏洞，迭代优化安全机制。
5. ​**安全意识（Safety Consciousness）​**：
    - 将伦理推理内化为模型核心能力，动态评估风险并生成低危害响应。

**终极挑战**：确保超级智能在所有场景中稳定遵循人类价值观。


#### ​**7.4 呼吁集体行动**

保障大模型免受对抗性操纵、滥用和危害是一项全球性挑战，需要研究者、从业者与政策制定者的共同努力。以下提出推动大模型安全的研究议程：

---

##### ​**7.4.1 防御导向的研究**

当前大模型安全研究严重偏向攻击策略，防御机制研究滞后于攻击技术的演进。为填补这一鸿沟，需调整研究优先级：

1. ​**平衡攻防研究**：不仅探索攻击机制，更需开发鲁棒防御方案。
2. ​**整合式防御**：新防御方法应与现有方案协同，构建分层防护体系。
3. ​**社区协作**：建立统一框架整合多样化防御机制，提升整体效能。

---

##### ​**7.4.2 专用安全API**

商业AI模型应提供**专用安全API**，支持研究者通过对抗与安全关键场景测试模型安全性，例如：

- 允许外部安全评估，不干扰用户服务。
- 促进产学研协作，推动模型安全持续改进。

---

##### ​**7.4.3 开源平台**

开发并开源安全平台与工具库，支持：

- ​**快速评估与测试**：跨模型与应用场景验证安全机制。
- ​**协作与透明化**：共享最佳实践，制定通用安全标准。

---

##### ​**7.4.4 全球协作**

AI安全超越国界，需学术界、企业、政府与非营利组织协同：

1. ​**国际安全联盟**：
    
    - 共享研究成果，协调安全评估，开发全球基准（兼容区域需求与价值观）。
2. ​**跨境数据共享框架**：
    
    - 构建安全伦理的数据共享机制，提升模型鲁棒性与公平性。
3. ​**联合研究计划**：
    
    - 聚焦对抗防御、安全对齐、实时适应性等方向，推动跨系统通用方案。
4. ​**全球安全竞赛**：
    
    - 组织国际挑战赛，激励创新解决方案，强化责任共担意识。
5. ​**政策协同**：
    
    - 协调各国AI治理政策，防止技术滥用，促进负责任开发。

**意义**：全球协作不仅提升安全研究效能，更增强透明度与可信度，确保AI技术惠及人类的同时最小化风险。

---

#### ​**总结**

通过防御研究转型、技术工具支持与全球协作机制，构建可持续的大模型安全生态。唯有集体行动，方能应对AI技术快速发展带来的复杂挑战。